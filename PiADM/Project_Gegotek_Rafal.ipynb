{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>A residual neural network - ResNet</center></h1>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Introduction</h2>\n",
    "\n",
    "<h3>1.1 Neural Networks</h3>\n",
    "\n",
    "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. Neural networks can adapt to changing input, so the network generates the best possible result without needing to redesign the output criteria.\n",
    "\n",
    "<img src=\"images/Simply_Neural_Network.png\" width=\"300\" height=\"300\">\n",
    "<center> Simply_Neural_Network </center>\n",
    "\n",
    "Neural networks are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts.\n",
    "\n",
    "<h3>1.2. VGG16 convolutional neural network</h3>\n",
    "\n",
    "One of the most popular models of nero networks is VGG16\n",
    "VGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. It was used to win the ILSVRC2014 competition in 2014. It still considered to be an excellent vision model.\n",
    "\n",
    "<img src=\"images/VGG16_architecture.png\" width=\"600\" height=\"400\">\n",
    "<center> Architecture of VGG16 networks </center>\n",
    "\n",
    "VGG-16 is a convolutional neural network that 16 layers deep. The model loads a set of weights pre-trained on ImageNet. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Resnet neural network</h2>\n",
    "\n",
    "<h3>2.1 Definition</h3>\n",
    "\n",
    "The ResNET network is a type of specialized neural network, specifically a typical example of a convolutional network that helps to handle more sophisticated deep learning tasks and models.\n",
    "The distinguishing feature of this network is the fact that it has the so-called residual blocks. Often can find signs like ResNET50 or ResNET152 (the numbers indicate the number of layers for a given network).\n",
    "\n",
    "The aforementioned residual blocks play a key role here, so it is worth explaining what role they play in this particular network.\n",
    "We know that neural networks are universal approximators of functions and that accuracy increases with increasing number of layers. However, there is a limit to the number of layers that can improve accuracy.\n",
    "\n",
    "So, if neural networks were universal approximators of functions, they should be able to learn any function. In practice, however, it turns out that the shallower webs learn better than their deeper counterparts for some time, which is quite counterintuitive.\n",
    "\n",
    "<h3>2.2 Residual blocks</h3>\n",
    "\n",
    "When we need to train a network that has many layers, it is a very difficult task. It is often the case that despite the addition of additional layers and iterations, the finally obtained network has worse parameters than in the case of a network trained with fewer layers and fewer iterations.\n",
    "\n",
    "As you might expect, an increase in the two mentioned before\n",
    "parameters has unfavorable consequences of such action. Namely, the extension of the learning time of the model, which in the described case is tantamount to its loss, and the already mentioned aspect has a worse trained model.\n",
    "\n",
    "The idea was to \"inject\" into the next layer, the signal from the previous layer and it's called 'identity shortcut connection'. It seems to be a complex, however, an explanation with the help of the model below will allow you to understand how simple this this mechanism of operation is.\n",
    "\n",
    "<img src=\"images/residual_block.png\" width=\"400\" height=\"400\">\n",
    "<center> Residual block structure </center>\n",
    "\n",
    "The above photo shows a single residual block used in ResNET networks. To explain the principle of action more clearly, let's adopt two scenarios.\n",
    "\n",
    "The first is the case where the layer weights are zero. Then also the output of such a block would show the value of the input signal, because the weights of these layers would not add anything new.\n",
    "\n",
    "On the other hand, if the weights of the layers taken into account are different from zero (and thus the F (x) signal as well), there is a chance that such a network with residues will learn something new, it can even be assumed from something correct, because it is on \"correct\" learning we care the most.\n",
    "\n",
    "However, it is worth emphasizing the phrase \"there is a chance\". Because at the design stage of this approach, no one could be sure that this approach would bring positive results, butr, the assumption was very simple. \n",
    "It was argued that introducing such a block (i.e. to the original signal from before additional weights, adding new weights) will not worsen the learning process, and on the contrary, it may be possible to gain something from it. Ultimately, this experimentation was successful, and made ResNet popular \n",
    "\n",
    "<h3>2.3 Additional information</h3>\n",
    "\n",
    "ResNet, as already mentioned, is a powerful type of neural network that is very often used in many tasks related to graphics. It's also already known that ResNet adds output from an earlier layer to a later one.\n",
    "\n",
    "This helps to alleviate the so-called disappearing gradient, which is responsible for the fact that often adding subsequent layers in typical neural networks does not improve the results, and even leads to the fact that the learned network has a lower efficiency. It is the vanishing gradient problem that is responsible for the fact that from a certain point we cannot increase the efficiency of a typical neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Practice</h2>\n",
    "\n",
    "In this part, the operation of both models of the discussed neural networks will be presented, using the tensorflow.keras library and the ImageNet database\n",
    "\n",
    "ImageNet is an image database with a total of 14 million images and 22 thousand visual categories. As it is publicly available for research and educational use, it has been widely used in the research of object recognition algorithms, and has played an important role in the deep learning revolution.\n",
    "ImageNet has been mostly used for researching object recognition algorithms on the subset of 1000 categories.\n",
    "\n",
    "<h3>3.1 VGG16 model</h3>\n",
    "\n",
    "In the case of the VGG16 network, we used a ready-made model that allows us to conduct tests without any problems.\n",
    "To do this, we have created the following function to retrieve a given image, process it for analysis, and then insert it into the model and get the value predicted by the model.\n",
    "This model gives as a result a tuple containing the names of the objects (i.e. objects, animals, food, etc.) that it was taught and the probability with which it predicted the choice of a specific image.\n",
    "Moreover, we can list more than one of such image-probability sets using the function top = n (where n is n images with the highest probability of matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of VGG16:\n",
      "object: strawberry.jpg =>  ('strawberry', 0.9970606)\n",
      "object: cheeseburger.jpg =>  ('bagel', 0.276235) ... ('cheeseburger', 0.27371728)\n",
      "object: toilet_tissue.jpg =>  ('toilet_tissue', 0.6716942)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras.preprocessing as keras_utils\n",
    "import tensorflow.keras.applications.vgg16 as vgg16\n",
    "import numpy as np\n",
    "\n",
    "modelVGG16 = vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "# modelVGG16.summary()\n",
    "\n",
    "def vgg16_network(image, model):\n",
    "    img = keras_utils.image.load_img(image, target_size=(224, 224))\n",
    "    x = keras_utils.image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    vgg16_input = vgg16.preprocess_input(x)\n",
    "    vgg16_features = model.predict(vgg16_input)\n",
    "    return vgg16.decode_predictions(vgg16_features, top=3)[0]\n",
    "\n",
    "objectName1 = 'strawberry.jpg'\n",
    "objectName2 = 'cheeseburger.jpg'\n",
    "objectName3 = 'toilet_tissue.jpg'\n",
    "print('Result of VGG16:')    \n",
    "vgg16Result1 = vgg16_network('photos/'+objectName1, modelVGG16)\n",
    "vgg16Result2 = vgg16_network('photos/'+objectName2, modelVGG16)\n",
    "vgg16Result3 = vgg16_network('photos/'+objectName3, modelVGG16)\n",
    "print(\"object: \" + objectName1 + \" => \",vgg16Result1[0][1:]) \n",
    "print(\"object: \" + objectName2 + \" => \",vgg16Result2[0][1:], '...', vgg16Result2[1][1:]) \n",
    "print(\"object: \" + objectName3 + \" => \",vgg16Result3[0][1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the example above, the model managed to recognize two of the three elements, making a mistake in the image of the cheeseburger tagging it as a bagel. It's worth taking into account that the second correct result differs by only 0.3%.\n",
    "\n",
    "<h3>3.2 ResNet model</h3>\n",
    "\n",
    "The same example as for the purpose of the VGG16 network was used as part of the imlemetation of the ResNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of ResNet:\n",
      "object: strawberry.jpg =>  ('strawberry', 0.99925846)\n",
      "object: cheeseburger.jpg =>  ('cheeseburger', 0.29881862)\n",
      "object: toilet_tissue.jpg =>  ('paper_towel', 0.55854785) ... ('toilet_tissue', 0.44143495)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.applications.resnet50 as resnet50\n",
    "\n",
    "modelresnet50 = resnet50.ResNet50(weights='imagenet')\n",
    "#modelresnet50.summary()\n",
    "\n",
    "def ResNet_network(image, model):\n",
    "    img = keras_utils.image.load_img(image, target_size=(224, 224))\n",
    "    x = keras_utils.image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    resnet_input = resnet50.preprocess_input(x)\n",
    "    resnet_features = model.predict(resnet_input)\n",
    "    return resnet50.decode_predictions(resnet_features, top=3)[0]\n",
    "\n",
    "print('Result of ResNet:')    \n",
    "resnetResult1 = ResNet_network(\"photos/\"+objectName1, modelresnet50)\n",
    "resnetResult2 = ResNet_network(\"photos/\"+objectName2, modelresnet50)\n",
    "resnetResult3 = ResNet_network(\"photos/\"+objectName3, modelresnet50)\n",
    "print(\"object: \" + objectName1 + \" => \",resnetResult1[0][1:]) \n",
    "print(\"object: \" + objectName2 + \" => \",resnetResult2[0][1:]) \n",
    "print(\"object: \" + objectName3 + \" => \",resnetResult3[0][1:], '...', resnetResult3[1][1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "W tym przypadku model miał również problemy z identyfikacją jednego z elementów, błędnie definiując chusteczkę toaletową jako ręcznik papierowy, co można uznać za mały błąd, biorąc pod uwagę podobieństwo elementów.\n",
    "\n",
    "<h3>3.3 Connnect of ResNet and VGG16 networks</h3>\n",
    "\n",
    "As shown in the two previous examples, the models for the same data had different prediction results, so you can think about improving the reseltates by combining the two networks so that the final result is as close as possible to the target result.\n",
    "\n",
    "The algorithm presented below is a simple example of averaging the results from combining these two types of neural networks, which should allow the assumed effect to be achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of hybrid (ResNet & VGG16):\n",
      "object: cheeseburger.jpg =>  ['cheeseburger', 0.2862679362297058]\n",
      "object: toilet_tissue.jpg =>  ['toilet_tissue', 0.5565645694732666]\n"
     ]
    }
   ],
   "source": [
    "def hybridNetowrk(resnet, vgg16):\n",
    "    result = [list(t) for t in resnet]\n",
    "    for item in vgg16:\n",
    "        clases = [res[0] for res in result]\n",
    "        if(item[0] in clases):\n",
    "            index = clases.index(item[0]) \n",
    "            result[index][2] = (result[index][2] + item[2]) / 2\n",
    "        else:\n",
    "            result.append(item)\n",
    "    result = sorted(result, key=lambda x: x[2], reverse=True)\n",
    "    return result\n",
    "\n",
    "hybrid1 = hybridNetowrk(resnetResult2, vgg16Result2)\n",
    "hybrid2 = hybridNetowrk(resnetResult3, vgg16Result3)\n",
    "print('Result of hybrid (ResNet & VGG16):')\n",
    "print(\"object: \" + objectName2 + \" => \", hybrid1[0][1:])\n",
    "print(\"object: \" + objectName3 + \" => \", hybrid2[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.4 Comparison performance of the networks</h3>\n",
    "\n",
    "In this part, all presented Nero network models will be tested on a set of sample data consisting of 44 elements, and then their results will be compared with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of test objects and model predictions:\n",
      "\n",
      "+------------------+------------------+----------+------------------+---------+------------------+----------+\n",
      "|      Object      |  Resnet predict  | Resnet % |  VGG16 predict   | VGG16 % |      Hybrid      | Hybrid % |\n",
      "+------------------+------------------+----------+------------------+---------+------------------+----------+\n",
      "|     plunger      |     plunger      |   1.0    |     plunger      | 0.99989 |     plunger      | 0.99994  |\n",
      "|     basenji      |     basenji      | 0.99922  |     basenji      | 0.96796 |     basenji      | 0.98359  |\n",
      "|    monastery     |      \u001b[0;31;40mchurch\u001b[0m      | 0.83119  |      \u001b[0;31;40mcastle\u001b[0m      | 0.45355 |      \u001b[0;31;40mchurch\u001b[0m      | 0.50693  |\n",
      "|     buckeye      |      \u001b[0;31;40mabacus\u001b[0m      | 0.97427  |      \u001b[0;31;40mabacus\u001b[0m      | 0.72811 |      \u001b[0;31;40mabacus\u001b[0m      | 0.85119  |\n",
      "|     crayfish     |     \u001b[0;31;40mcricket\u001b[0m      | 0.62621  |     \u001b[0;31;40mcricket\u001b[0m      | 0.67055 |     \u001b[0;31;40mcricket\u001b[0m      | 0.64838  |\n",
      "|      screw       |      screw       |  0.9781  |      screw       | 0.94433 |      screw       | 0.96122  |\n",
      "|      llama       |      llama       | 0.99462  |      llama       | 0.99956 |      llama       | 0.99709  |\n",
      "|      hammer      |      hammer      | 0.98879  |      hammer      |  0.8983 |      hammer      | 0.94354  |\n",
      "|      church      |      church      | 0.72387  |      church      | 0.37813 |      church      |  0.551   |\n",
      "|    car_wheel     |    car_wheel     | 0.93773  |    car_wheel     | 0.61213 |    car_wheel     | 0.77493  |\n",
      "| combination_lock | combination_lock | 0.99773  | combination_lock | 0.99982 | combination_lock | 0.99878  |\n",
      "|   black_grouse   |   black_grouse   | 0.99824  |   black_grouse   | 0.99996 |   black_grouse   |  0.9991  |\n",
      "|    strawberry    |    strawberry    | 0.99926  |    strawberry    | 0.99706 |    strawberry    | 0.99816  |\n",
      "|    can_opener    |    can_opener    | 0.97917  |    can_opener    | 0.62534 |    can_opener    | 0.80225  |\n",
      "| Indian_elephant  | Indian_elephant  | 0.83252  | Indian_elephant  | 0.84457 | Indian_elephant  | 0.83854  |\n",
      "|     sombrero     |     sombrero     | 0.99988  |     sombrero     | 0.99868 |     sombrero     | 0.99928  |\n",
      "|     printer      |     printer      | 0.96931  |     printer      | 0.98999 |     printer      | 0.97965  |\n",
      "|    overskirt     |       \u001b[0;31;40mgown\u001b[0m       |  0.7202  |       \u001b[0;31;40mgown\u001b[0m       | 0.53627 |       \u001b[0;31;40mgown\u001b[0m       | 0.62824  |\n",
      "|     tractor      |     tractor      | 0.90552  |     tractor      | 0.87416 |     tractor      | 0.88984  |\n",
      "|    bell_cote     |    bell_cote     | 0.91662  |    bell_cote     | 0.99761 |    bell_cote     | 0.95711  |\n",
      "|    spider_web    |    spider_web    |  0.9995  |    spider_web    | 0.99715 |    spider_web    | 0.99833  |\n",
      "|    bald_eagle    |    bald_eagle    | 0.99806  |    bald_eagle    | 0.99995 |    bald_eagle    | 0.99901  |\n",
      "|     wardrobe     |     wardrobe     | 0.95599  |     wardrobe     | 0.59779 |     wardrobe     | 0.77689  |\n",
      "|       kite       |     \u001b[0;31;40mweb_site\u001b[0m     |  0.5266  |     \u001b[0;31;40mweb_site\u001b[0m     | 0.45705 |     \u001b[0;31;40mweb_site\u001b[0m     | 0.49182  |\n",
      "|     perfume      |     perfume      | 0.99994  |     perfume      | 0.99921 |     perfume      | 0.99957  |\n",
      "|     volcano      |     volcano      |  0.9999  |     volcano      | 0.99237 |     volcano      | 0.99614  |\n",
      "|   cheeseburger   |   cheeseburger   | 0.29882  |      \u001b[0;31;40mbagel\u001b[0m       | 0.27624 |   cheeseburger   | 0.28627  |\n",
      "|  toilet_tissue   |   \u001b[0;31;40mpaper_towel\u001b[0m    | 0.55855  |  toilet_tissue   | 0.67169 |  toilet_tissue   | 0.55656  |\n",
      "|   tiger_shark    |   tiger_shark    | 0.99869  |   tiger_shark    |  0.9932 |   tiger_shark    | 0.99594  |\n",
      "|   grand_piano    |   grand_piano    | 0.99971  |   grand_piano    | 0.99745 |   grand_piano    | 0.99858  |\n",
      "|      teapot      |    \u001b[0;31;40mcoffeepot\u001b[0m     | 0.70673  |    \u001b[0;31;40mcoffeepot\u001b[0m     | 0.63707 |    \u001b[0;31;40mcoffeepot\u001b[0m     |  0.6719  |\n",
      "|    carbonara     |    carbonara     | 0.99983  |    carbonara     | 0.98674 |    carbonara     | 0.99329  |\n",
      "|   window_shade   |   window_shade   | 0.90388  |   window_shade   | 0.29768 |   window_shade   | 0.60078  |\n",
      "|  French_bulldog  |  French_bulldog  | 0.97435  |  French_bulldog  | 0.95504 |  French_bulldog  | 0.96469  |\n",
      "|      badger      |      badger      | 0.99999  |      badger      | 0.99999 |      badger      | 0.99999  |\n",
      "|     minivan      |     \u001b[0;31;40mminibus\u001b[0m      | 0.38602  |     \u001b[0;31;40mminibus\u001b[0m      | 0.48865 |     \u001b[0;31;40mminibus\u001b[0m      | 0.43734  |\n",
      "|     seashore     |     seashore     | 0.66543  |   \u001b[0;31;40mhandkerchief\u001b[0m   | 0.21837 |     seashore     | 0.37339  |\n",
      "|     scorpion     |     scorpion     | 0.99996  |     scorpion     | 0.99614 |     scorpion     | 0.99805  |\n",
      "|    sports_car    |    sports_car    | 0.28866  |    sports_car    | 0.15635 |    sports_car    |  0.2225  |\n",
      "|      banana      |      banana      | 0.95599  |      banana      | 0.34891 |      banana      | 0.65245  |\n",
      "|  mountain_bike   |  mountain_bike   | 0.67258  |  mountain_bike   | 0.94526 |  mountain_bike   | 0.80892  |\n",
      "|     mashroom     | \u001b[0;31;40mhen-of-the-woods\u001b[0m | 0.26394  |      \u001b[0;31;40mdough\u001b[0m       | 0.37725 |      \u001b[0;31;40mdough\u001b[0m       | 0.37725  |\n",
      "|      hamper      |      hamper      | 0.99315  |      hamper      | 0.97788 |      hamper      | 0.98552  |\n",
      "|   cauliflower    |   cauliflower    | 0.99989  |   cauliflower    | 0.99997 |   cauliflower    | 0.99993  |\n",
      "+------------------+------------------+----------+------------------+---------+------------------+----------+ \n",
      "\n",
      "SUMMARY:\n",
      " +--------------+----------+-------------+---------+--------+----------+\n",
      "| Resnet Score | Resnet % | VGG16 score | VGG16 % | Hybrid | Hybrid % |\n",
      "+--------------+----------+-------------+---------+--------+----------+\n",
      "|      35      |  0.8527  |      34     | 0.77273 |   36   | 0.80668  |\n",
      "+--------------+----------+-------------+---------+--------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from prettytable import PrettyTable\n",
    "import progressbar\n",
    "\n",
    "directory = 'photos/'\n",
    "resNet_score, vgg16_score, hybrid_score = 0, 0, 0\n",
    "resNet_sum, vgg16_sum, hybrid_sum = 0, 0, 0\n",
    "iteration  = 0\n",
    "table = PrettyTable(['Object', 'Resnet predict', 'Resnet %', 'VGG16 predict', 'VGG16 %', 'Hybrid', 'Hybrid %'])\n",
    "size = len(os.listdir(directory))\n",
    "bar = progressbar.ProgressBar(maxval=size, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "bar.start()\n",
    "for filename in os.listdir(directory):\n",
    "    bar.update(iteration+1)\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        iteration += 1\n",
    "        resNet_predict = ResNet_network(directory+filename, modelresnet50)\n",
    "        vgg16_predict  = vgg16_network(directory+filename, modelVGG16)\n",
    "        hybrid_predict = hybridNetowrk(resNet_predict, vgg16_predict)\n",
    "        resnet_val = resNet_predict[0][1]\n",
    "        vggg16_val = vgg16_predict[0][1]\n",
    "        hybrid_val = hybrid_predict[0][1]\n",
    "        resnet_percentage = round(resNet_predict[0][2],5)\n",
    "        vggg16_percentage = round(vgg16_predict[0][2],5)\n",
    "        hybrid_percentage = round(hybrid_predict[0][2],5)\n",
    "        if (resnet_val == filename.split('.')[0]):  \n",
    "            resNet_score += 1\n",
    "        else:\n",
    "            resnet_val = \"\\033[0;31;40m\" + resnet_val + \"\\033[0m\"\n",
    "        if (vggg16_val == filename.split('.')[0]):   \n",
    "            vgg16_score  += 1\n",
    "        else:\n",
    "            vggg16_val = \"\\033[0;31;40m\" + vggg16_val + \"\\033[0m\"\n",
    "        if (hybrid_val == filename.split('.')[0]):  \n",
    "            hybrid_score += 1\n",
    "        else:\n",
    "            hybrid_val = \"\\033[0;31;40m\" + hybrid_val + \"\\033[0m\"\n",
    "        table.add_row([filename.split('.')[0], resnet_val, resnet_percentage, vggg16_val, \\\n",
    "                       vggg16_percentage, hybrid_val, hybrid_percentage])\n",
    "        resNet_sum += resnet_percentage\n",
    "        vgg16_sum  += vggg16_percentage\n",
    "        hybrid_sum += hybrid_percentage\n",
    "    else:\n",
    "        continue\n",
    "bar.finish()\n",
    "\n",
    "print(\"Table of test objects and model predictions:\\n\")\n",
    "summary = PrettyTable(['Resnet Score', 'Resnet %', 'VGG16 score', 'VGG16 %', 'Hybrid', 'Hybrid %'])\n",
    "summary.add_row([resNet_score, round(resNet_sum/iteration,5), vgg16_score, \\\n",
    "                 round(vgg16_score/iteration,5), hybrid_score, round(hybrid_sum/iteration,5)])\n",
    "print(table, \"\\n\\nSUMMARY:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Conclusions</h2>\n",
    "\n",
    "As presented in item no. 3, the VGG16 model with only 16 layers did as well as the ResNET50 network. In case of mistakes made, a lot of them are common. However, there are exceptions. On this basis, after careful analysis, one could try to make assumptions with which images a given network cope better and with which it is worse.\n",
    "\n",
    "An interesting observation is the fact that, as far as the percentage of hits was the same, to the disadvantage of ResNET50 in this case, the fact is that he was much more \"sure\" that he chose well, even though he chose wrong. Probability values were usually not lower than 0.7 for the image predicted by the model. In the case of the VGG16 model, the value of the hit probability was much lower and even values of the order of 0.2 were found. This means that the VGG16 model had \"more doubts\" than the ResNET50 model in case of wrong choice.\n",
    "\n",
    "Considering the combination of both neural networks as one model, the intended effect was obtained. Ultimately, this model had the most well-known objects in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
