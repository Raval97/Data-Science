{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0) Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape = (25000, 10000)\n",
      "X_train.shape = (25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "def data_prepare(sequences):\n",
    "    res = np.zeros(shape=(len(sequences), 10001), dtype=int)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        res[i, np.array(sequence)] = 1\n",
    "    return res[:, 1:].copy()\n",
    "\n",
    "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(\n",
    "    num_words = 10001,\n",
    "    start_char = 0,\n",
    "    oov_char = 0,\n",
    "    index_from = 0\n",
    ")\n",
    "\n",
    "X_test = data_prepare(X_test_raw)\n",
    "X_train = data_prepare(X_train_raw)\n",
    "\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"X_train.shape =\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1) Base Model (from lab 6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer3 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model1.add(layer1)\n",
    "model1.add(layer2)\n",
    "model1.add(layer3)\n",
    "model1.add(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model & plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "782/782 [==============================] - 27s 12ms/step - loss: 0.3805 - accuracy: 0.8323 - val_loss: 0.3034 - val_accuracy: 0.8800\n",
      "Epoch 2/8\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1980 - accuracy: 0.9282 - val_loss: 0.3402 - val_accuracy: 0.8751\n",
      "Epoch 3/8\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.1535 - accuracy: 0.9448 - val_loss: 0.3504 - val_accuracy: 0.8734\n",
      "Epoch 4/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1131 - accuracy: 0.9632 - val_loss: 0.3617 - val_accuracy: 0.8714\n",
      "Epoch 5/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0773 - accuracy: 0.9768 - val_loss: 0.4720 - val_accuracy: 0.8693\n",
      "Epoch 6/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0535 - accuracy: 0.9863 - val_loss: 0.6049 - val_accuracy: 0.8648\n",
      "Epoch 7/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0327 - accuracy: 0.9926 - val_loss: 0.7576 - val_accuracy: 0.8678\n",
      "Epoch 8/8\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.9122 - val_accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, y_train, epochs=8, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2) Model in early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "782/782 [==============================] - 10s 11ms/step - loss: 0.3864 - accuracy: 0.8279 - val_loss: 0.2906 - val_accuracy: 0.8842\n",
      "Epoch 2/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2026 - accuracy: 0.9247 - val_loss: 0.3007 - val_accuracy: 0.8839\n",
      "Epoch 3/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1503 - accuracy: 0.9468 - val_loss: 0.3226 - val_accuracy: 0.8777\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer3 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model2.add(layer1)\n",
    "model2.add(layer2)\n",
    "model2.add(layer3)\n",
    "model2.add(output_layer)\n",
    "\n",
    "model2.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, epochs=8, validation_data=(X_test, y_test), callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3) Model in early stopping\n",
    "# and batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "782/782 [==============================] - 11s 12ms/step - loss: 0.3910 - accuracy: 0.8232 - val_loss: 0.3202 - val_accuracy: 0.8706\n",
      "Epoch 2/8\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.1849 - accuracy: 0.9307 - val_loss: 0.3200 - val_accuracy: 0.8728\n",
      "Epoch 3/8\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1131 - accuracy: 0.9605 - val_loss: 0.4057 - val_accuracy: 0.8612\n",
      "Epoch 4/8\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.0553 - accuracy: 0.9813 - val_loss: 0.5120 - val_accuracy: 0.8515\n"
     ]
    }
   ],
   "source": [
    "model3 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer4 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model3.add(layer1)\n",
    "model3.add(layer2)\n",
    "model3.add(layer3)\n",
    "model3.add(layer4)\n",
    "model3.add(output_layer)\n",
    "\n",
    "model3.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop_cb2 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, epochs=8, validation_data=(X_test, y_test), callbacks=[early_stop_cb2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1563/1563 [==============================] - 17s 10ms/step - loss: 0.4125 - accuracy: 0.8129 - val_loss: 0.2959 - val_accuracy: 0.8797\n",
      "Epoch 2/8\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2290 - accuracy: 0.9116 - val_loss: 0.3229 - val_accuracy: 0.8685\n",
      "Epoch 3/8\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1582 - accuracy: 0.9412 - val_loss: 0.3785 - val_accuracy: 0.8656\n"
     ]
    }
   ],
   "source": [
    "model4 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer4 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model4.add(layer1)\n",
    "model4.add(layer2)\n",
    "model4.add(layer3)\n",
    "model4.add(layer4)\n",
    "model4.add(output_layer)\n",
    "\n",
    "model4.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop_cb3 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history4 = model4.fit(X_train, y_train, batch_size=16, epochs=8, validation_data=(X_test, y_test), callbacks=[early_stop_cb3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4) Model in early stopping \n",
    "# and batch normalization of all hidden layers in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "782/782 [==============================] - 12s 12ms/step - loss: 0.4230 - accuracy: 0.8045 - val_loss: 0.2881 - val_accuracy: 0.8806\n",
      "Epoch 2/8\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.2016 - accuracy: 0.9217 - val_loss: 0.3034 - val_accuracy: 0.8754\n",
      "Epoch 3/8\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.1230 - accuracy: 0.9593 - val_loss: 0.3713 - val_accuracy: 0.8636\n"
     ]
    }
   ],
   "source": [
    "model5 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer4 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer5 = keras.layers.Dense(units=10, activation='relu')\n",
    "layer6 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model5.add(layer1)\n",
    "model5.add(layer2)\n",
    "model5.add(layer3)\n",
    "model5.add(layer4)\n",
    "model5.add(layer5)\n",
    "model5.add(layer6)\n",
    "model5.add(output_layer)\n",
    "\n",
    "model5.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop_cb4 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history5 = model5.fit(X_train, y_train, epochs=8, validation_data=(X_test, y_test), callbacks=[early_stop_cb4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5) Model in early stopping \n",
    "# and batch normalization before RLU activation of each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "782/782 [==============================] - 12s 13ms/step - loss: 0.4175 - accuracy: 0.8105 - val_loss: 0.3054 - val_accuracy: 0.8727\n",
      "Epoch 2/8\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1940 - accuracy: 0.9274 - val_loss: 0.3180 - val_accuracy: 0.8704\n",
      "Epoch 3/8\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 0.1223 - accuracy: 0.9581 - val_loss: 0.4119 - val_accuracy: 0.8496\n"
     ]
    }
   ],
   "source": [
    "model6 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100)\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.ReLU()\n",
    "layer4 = keras.layers.Dense(units=50)\n",
    "layer5 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer6 = keras.layers.ReLU()\n",
    "layer7 = keras.layers.Dense(units=10)\n",
    "layer8 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer9 = keras.layers.ReLU()\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model6.add(layer1)\n",
    "model6.add(layer2)\n",
    "model6.add(layer3)\n",
    "model6.add(layer4)\n",
    "model6.add(layer5)\n",
    "model6.add(layer6)\n",
    "model6.add(layer7)\n",
    "model6.add(layer8)\n",
    "model6.add(layer9)\n",
    "model6.add(output_layer)\n",
    "\n",
    "model6.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stop_cb5 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history6 = model6.fit(X_train, y_train, epochs=8,validation_data=(X_test, y_test), callbacks=[early_stop_cb5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model (from lab 6):\n",
      "Train: 0.9976000189781189 \tTest: 0.8645200133323669 \n",
      "\n",
      "Model in early stopping:\n",
      "Train: 0.9330400228500366 \tTest: 0.8841599822044373 \n",
      "\n",
      "Model in early stopping and batch normalization:\n",
      "Batch size of 32:\n",
      "Train: 0.9693599939346313 \tTest: 0.8728399872779846\n",
      "Batch size of 16:\n",
      "Train: 0.9323199987411499 \tTest: 0.8796799778938293 \n",
      "\n",
      "Model in early stopping and batch normalization of all hidden layers in the network:\n",
      "Train: 0.9373599886894226 \tTest: 0.8805599808692932 \n",
      "\n",
      "Model in early stopping and batch normalization before RLU activation of each hidden layer:\n",
      "Train: 0.937279999256134 \tTest: 0.8727200031280518 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, trainAcc1 = model1.evaluate(X_train, y_train, verbose=0)\n",
    "_, testAcc1 = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print('Base Model (from lab 6):')\n",
    "print('Train:', trainAcc1,'\\tTest:',testAcc1,'\\n')\n",
    "\n",
    "_, trainAcc2 = model2.evaluate(X_train, y_train, verbose=0)\n",
    "_, testAcc2 = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print('Model in early stopping:')\n",
    "print('Train:', trainAcc2,'\\tTest:',testAcc2,'\\n')\n",
    "\n",
    "\n",
    "print('Model in early stopping and batch normalization:')\n",
    "_, trainAcc3 = model3.evaluate(X_train, y_train, verbose=0)\n",
    "_, testAcc3 = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print('Batch size of 32:\\nTrain:', trainAcc3,'\\tTest:',testAcc3)\n",
    "_, trainAcc4 = model4.evaluate(X_train, y_train, verbose=0)\n",
    "_, testAcc4 = model4.evaluate(X_test, y_test, verbose=0)\n",
    "print('Batch size of 16:\\nTrain:', trainAcc4,'\\tTest:',testAcc4,'\\n')\n",
    "\n",
    "\n",
    "_, trainAcc5 = model5.evaluate(X_train, y_train, verbose=0)\n",
    "_, testAcc5 = model5.evaluate(X_test, y_test, verbose=0)\n",
    "print('Model in early stopping and batch normalization of all hidden layers in the network:')\n",
    "print('Train:', trainAcc5,'\\tTest:',testAcc5,'\\n')\n",
    "\n",
    "\n",
    "_, trainAcc6 = model6.evaluate(X_train, y_train, verbose=0)\n",
    "_, testAcc6 = model6.evaluate(X_test, y_test, verbose=0)\n",
    "print('Model in early stopping and batch normalization before RLU activation of each hidden layer:')\n",
    "print('Train:', trainAcc6,'\\tTest:',testAcc6,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic model achieved best for training data and lowest for test data relative to the rest. \\\n",
    "All other models, thanks to the use of early stopping, allowed for faster model creation, because they did not need to use all epochs, usually 3 epochs were enough.Additionally, the performance of the model for test data has increased. \\\n",
    "The use of batch normalization and use it for all hidden layer befoe or after activation does not cause significant differences in effectiveness for both training and testing data.  \\\n",
    "One can only observe that for smaller amounts of batch size (like 16) the results are worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
