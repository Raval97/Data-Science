{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXPUlLg7PHAF"
   },
   "source": [
    "**TASK:**\n",
    "\n",
    "Neural Networks and Deep Learning\n",
    "Cracow University of Technology\n",
    "\n",
    "Lab Assignment 5:\n",
    "\n",
    "The purpose of this laboratory is to implement a neural network for a classification task:\n",
    "\n",
    "\n",
    "\n",
    "1.   The network is trained using minibatch stochastic gradient descent.\n",
    "2.   You have images of handwritten digits from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and you should train the network to predict the value of the digit for images.\n",
    "\n",
    "Network specification:\n",
    "\n",
    "1.   Input layer - one hidden layer - output layer\n",
    "2.   Activation functions: for hidden layer \"ReLU\" and for output layer \"softmax\"\n",
    "3.   Loss function: categorical cross-entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s8Mq_ZnWWVzU"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_ = np.exp(x)\n",
    "    return exp_ / np.sum(exp_, axis=0)\n",
    "\n",
    "def d_softmax(x):\n",
    "    y = softmax(x)\n",
    "    n = y.shape[1]\n",
    "    tiled = np.tile(y, (10, 1, 1))\n",
    "    return tiled * (np.diag([1]*10)[..., np.newaxis]-tiled.transpose(1, 0, 2))\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x > 0.0, x, 0.0)\n",
    "\n",
    "def D_relu(x):\n",
    "    return np.where(x > 0.0, 1.0, 0.0)\n",
    "\n",
    "def loss(predicted, target):\n",
    "    return -np.mean(np.sum(target*np.log(predicted), axis=0))\n",
    "\n",
    "def d_loss(predicted, target):\n",
    "    return - target / predicted / predicted.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7FFYLpnf6oT"
   },
   "source": [
    "Your code consists of at least five functions:\n",
    "\n",
    "* Network initialization\n",
    "* Forward pass\n",
    "* Backward pass\n",
    "* Train \n",
    "* Evaluate\n",
    "\n",
    "You are free to add more functions for the sake of having better organization for your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7VByNZMXsNa"
   },
   "source": [
    "Tune your network by changing hyperparametes of the network:\n",
    "* Number of epochs\n",
    "* Number of neurons in hidden layer\n",
    "* Different learning rates\n",
    "* Different minibatch sizes\n",
    "\n",
    "Also, try the following changes to the network:\n",
    "* Apply different optimziation algorithms: Momentum, Adagrad, RMSprop, and ADAM\n",
    "* Apply L2 regularization techniques to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcIVzaTuWT5H"
   },
   "source": [
    "Please submit your code with report on the error rate. You can also compare your results with the MNIST performance results exists on the MNIST website.\n",
    "Please also report the effect of different changes you made in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 784)\n",
      "X_test: (10000, 784)\n",
      "y_train: (60000, 10)\n",
      "y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "def read_data(path):\n",
    "    if 'images' in path:\n",
    "        elem_size, header_bytes = 28, 16\n",
    "        type_ = np.float32\n",
    "    else:\n",
    "        elem_size, header_bytes = 1, 8\n",
    "        type_ = np.uint8        \n",
    "    if 't10k' in path:\n",
    "        num = 10000\n",
    "    else:\n",
    "        num = 60000      \n",
    "    f = gzip.open(path, 'r')\n",
    "    f.read(header_bytes)\n",
    "    shape = 1 if elem_size == 1 else (elem_size, elem_size)\n",
    "    return np.array([\n",
    "        np.frombuffer(\n",
    "            f.read(elem_size*elem_size), \n",
    "            dtype=np.uint8\n",
    "        ).astype(type_).reshape(shape)\n",
    "        for _ in range(num)\n",
    "    ])\n",
    "\n",
    "def labels_to_one_hot(array):\n",
    "    n = array.shape[0]\n",
    "    res = np.zeros((n, 10))\n",
    "    res[np.arange(n), array.ravel()] = 1\n",
    "    return res\n",
    "\n",
    "X_train = read_data(\"dataSet/train-images-idx3-ubyte.gz\").reshape(-1, 784) / 255.\n",
    "X_test = read_data(\"dataSet/t10k-images-idx3-ubyte.gz\").reshape(-1, 784) / 255.\n",
    "y_train = labels_to_one_hot(read_data(\"dataSet/train-labels-idx1-ubyte.gz\"))\n",
    "y_test = labels_to_one_hot(read_data(\"dataSet/t10k-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkWeights:\n",
    "    def __init__(self, W1, b1, W2, b2):\n",
    "        self.W1 = W1\n",
    "        self.b1 = b1\n",
    "        self.W2 = W2\n",
    "        self.b2 = b2\n",
    "        \n",
    "def network_initialization(n_hidden_units):\n",
    "    Weight1 = np.random.normal(scale=np.sqrt(2.0/784),loc=0.0,size=(n_hidden_units, 784))\n",
    "    Weight2 = np.random.normal(scale=np.sqrt(2.0/(n_hidden_units+10)),loc=0.0,size=(10, n_hidden_units))\n",
    "    bias1 = np.zeros((n_hidden_units, 1))\n",
    "    bias2 = np.zeros((10, 1))\n",
    "    return NetworkWeights(Weight1, bias1, Weight2, bias2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(weights, X):\n",
    "    z1 = np.matmul(weights.W1, X.T) + weights.b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.matmul(weights.W2, a1) + weights.b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(weights, X, y):\n",
    "    z1, a1, z2, a2 = forward_pass(weights, X)\n",
    "    \n",
    "    L = loss(a2, y.T)\n",
    "    dL = np.matmul(d_softmax(z2).T, d_loss(a2, y.T)[np.newaxis, ...].T).T[0, ...]\n",
    "    grad_W2 = np.matmul(dL, a1.T)\n",
    "    grad_b2 = np.mean(dL, axis=1, keepdims=True)\n",
    "    grad_W1 = np.matmul(np.matmul(weights.W2.T, dL) * D_relu(a1), X)\n",
    "    grad_b1 = np.mean(np.matmul(weights.W2.T, dL) * D_relu(a1), axis=1, keepdims=True)\n",
    "    \n",
    "    return NetworkWeights(grad_W1, grad_b1, grad_W2, grad_b2), L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y, weights):\n",
    "    _, _, _, a2 = forward_pass(weights, X)\n",
    "    L = loss(a2, y.T)\n",
    "    a2_ = np.argmax(a2.T, axis=1)\n",
    "    y_ = np.argmax(y, axis=1)\n",
    "    print(f'Evaluation:\\tLoss: {L:.6f}\\tAccuracy: {np.mean(a2_ == y_):.6f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(X, y, weights, l_rate):\n",
    "    grads, L = backward_pass(weights, X, y)  \n",
    "    weights.W2 -= grads.W2 * l_rate\n",
    "    weights.b2 -= grads.b2 * l_rate\n",
    "    weights.W1 -= grads.W1 * l_rate\n",
    "    weights.b1 -= grads.b1 * l_rate\n",
    "    return L\n",
    "\n",
    "def train(X, y, weights, l_rate, epochs, batch_size):\n",
    "    n_total = X.shape[0]\n",
    "    n_batches = n_total // batch_size + (1 if n_total % batch_size != 0 else 0)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        L = 0.0\n",
    "        for n_batch in range(1, n_batches+1):\n",
    "            batch_X = X[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            batch_y = y[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            L += train_one_batch(batch_X, batch_y, weights, l_rate)\n",
    "            print('\\rEpoch: {}\\tBatch: {}/{}\\tLoss: {:.6f}\\t'.format(epoch, n_batch, n_batches, L/n_batch), end='')\n",
    "        print('\\rEpoch: {}\\t'.format(epoch), end='')\n",
    "        evaluate(X, y, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_train(n_hidden_units, epochs, l_rate, batch_size):\n",
    "    weights = network_initialization(n_hidden_units)\n",
    "    train(X_train, y_train, weights, l_rate, epochs, batch_size)\n",
    "    evaluate(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========  n_hidden_units = 100\tepochs = 5\tl_rate = 0.2\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.209165\tEvaluation:\tLoss: 0.129576\tAccuracy: 0.959933\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.101224\tEvaluation:\tLoss: 0.101794\tAccuracy: 0.967150\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.070838\tEvaluation:\tLoss: 0.084868\tAccuracy: 0.971483\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.054391\tEvaluation:\tLoss: 0.073139\tAccuracy: 0.976367\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.042083\tEvaluation:\tLoss: 0.054448\tAccuracy: 0.982050\n",
      "Evaluation:\tLoss: 0.113353\tAccuracy: 0.969800\n",
      "===========  n_hidden_units = 100\tepochs = 5\tl_rate = 0.2\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.240942\tEvaluation:\tLoss: 0.142644\tAccuracy: 0.955850\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.111585\tEvaluation:\tLoss: 0.091079\tAccuracy: 0.971650\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.079606\tEvaluation:\tLoss: 0.068745\tAccuracy: 0.978567\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.061621\tEvaluation:\tLoss: 0.056409\tAccuracy: 0.982000\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.049375\tEvaluation:\tLoss: 0.046722\tAccuracy: 0.985350\n",
      "Evaluation:\tLoss: 0.087270\tAccuracy: 0.972800\n",
      "===========  n_hidden_units = 100\tepochs = 5\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.505995\tEvaluation:\tLoss: 0.319891\tAccuracy: 0.908067\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.281975\tEvaluation:\tLoss: 0.255083\tAccuracy: 0.927500\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.233038\tEvaluation:\tLoss: 0.216287\tAccuracy: 0.939167\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.200633\tEvaluation:\tLoss: 0.188438\tAccuracy: 0.947050\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.176945\tEvaluation:\tLoss: 0.167278\tAccuracy: 0.952867\n",
      "Evaluation:\tLoss: 0.168779\tAccuracy: 0.951000\n",
      "===========  n_hidden_units = 100\tepochs = 5\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.650478\tEvaluation:\tLoss: 0.378175\tAccuracy: 0.893667\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.338402\tEvaluation:\tLoss: 0.309396\tAccuracy: 0.912283\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.290478\tEvaluation:\tLoss: 0.272958\tAccuracy: 0.924067\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.260071\tEvaluation:\tLoss: 0.246806\tAccuracy: 0.932000\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.237219\tEvaluation:\tLoss: 0.226507\tAccuracy: 0.937600\n",
      "Evaluation:\tLoss: 0.223437\tAccuracy: 0.936800\n",
      "===========  n_hidden_units = 100\tepochs = 10\tl_rate = 0.2\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.213613\tEvaluation:\tLoss: 0.143261\tAccuracy: 0.953983\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.099413\tEvaluation:\tLoss: 0.100447\tAccuracy: 0.967683\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.070967\tEvaluation:\tLoss: 0.070564\tAccuracy: 0.977100\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.052722\tEvaluation:\tLoss: 0.052542\tAccuracy: 0.982283\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.041024\tEvaluation:\tLoss: 0.051236\tAccuracy: 0.982433\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.032623\tEvaluation:\tLoss: 0.053067\tAccuracy: 0.982550\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.026928\tEvaluation:\tLoss: 0.040047\tAccuracy: 0.986117\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.022593\tEvaluation:\tLoss: 0.041188\tAccuracy: 0.986450\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.017901\tEvaluation:\tLoss: 0.029709\tAccuracy: 0.989983\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.017358\tEvaluation:\tLoss: 0.047037\tAccuracy: 0.984483\n",
      "Evaluation:\tLoss: 0.144092\tAccuracy: 0.968200\n",
      "===========  n_hidden_units = 100\tepochs = 10\tl_rate = 0.2\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.242352\tEvaluation:\tLoss: 0.140627\tAccuracy: 0.957250\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.108600\tEvaluation:\tLoss: 0.095516\tAccuracy: 0.969550\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.078198\tEvaluation:\tLoss: 0.072175\tAccuracy: 0.977717\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.060331\tEvaluation:\tLoss: 0.059681\tAccuracy: 0.981133\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.047816\tEvaluation:\tLoss: 0.053635\tAccuracy: 0.982350\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.038568\tEvaluation:\tLoss: 0.044813\tAccuracy: 0.985250\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.031067\tEvaluation:\tLoss: 0.037146\tAccuracy: 0.987900\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.025104\tEvaluation:\tLoss: 0.030499\tAccuracy: 0.990417\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.020368\tEvaluation:\tLoss: 0.026678\tAccuracy: 0.991783\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.016074\tEvaluation:\tLoss: 0.023882\tAccuracy: 0.992750\n",
      "Evaluation:\tLoss: 0.079794\tAccuracy: 0.976700\n",
      "===========  n_hidden_units = 100\tepochs = 10\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.501778\tEvaluation:\tLoss: 0.320520\tAccuracy: 0.906600\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.281219\tEvaluation:\tLoss: 0.255912\tAccuracy: 0.926233\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.232608\tEvaluation:\tLoss: 0.216736\tAccuracy: 0.937883\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.200585\tEvaluation:\tLoss: 0.189160\tAccuracy: 0.945933\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.177415\tEvaluation:\tLoss: 0.168594\tAccuracy: 0.951800\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.159534\tEvaluation:\tLoss: 0.152435\tAccuracy: 0.956717\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.145292\tEvaluation:\tLoss: 0.139451\tAccuracy: 0.960400\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.133581\tEvaluation:\tLoss: 0.128539\tAccuracy: 0.963567\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.123758\tEvaluation:\tLoss: 0.119457\tAccuracy: 0.965983\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.115371\tEvaluation:\tLoss: 0.111494\tAccuracy: 0.968133\n",
      "Evaluation:\tLoss: 0.124422\tAccuracy: 0.962400\n",
      "===========  n_hidden_units = 100\tepochs = 10\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.662600\tEvaluation:\tLoss: 0.384275\tAccuracy: 0.891350\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.342040\tEvaluation:\tLoss: 0.311415\tAccuracy: 0.911200\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.291429\tEvaluation:\tLoss: 0.273490\tAccuracy: 0.922567\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.260072\tEvaluation:\tLoss: 0.246993\tAccuracy: 0.930350\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.236938\tEvaluation:\tLoss: 0.226505\tAccuracy: 0.936733\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.218610\tEvaluation:\tLoss: 0.209754\tAccuracy: 0.941267\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.203408\tEvaluation:\tLoss: 0.195678\tAccuracy: 0.945183\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.190472\tEvaluation:\tLoss: 0.183555\tAccuracy: 0.948517\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.179211\tEvaluation:\tLoss: 0.172939\tAccuracy: 0.951367\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.169387\tEvaluation:\tLoss: 0.163633\tAccuracy: 0.954450\n",
      "Evaluation:\tLoss: 0.167750\tAccuracy: 0.952300\n",
      "===========  n_hidden_units = 200\tepochs = 5\tl_rate = 0.2\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.198316\tEvaluation:\tLoss: 0.115664\tAccuracy: 0.962883\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.085964\tEvaluation:\tLoss: 0.073932\tAccuracy: 0.976050\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.057869\tEvaluation:\tLoss: 0.053884\tAccuracy: 0.981783\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.040969\tEvaluation:\tLoss: 0.040656\tAccuracy: 0.986783\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.028353\tEvaluation:\tLoss: 0.034092\tAccuracy: 0.988717\n",
      "Evaluation:\tLoss: 0.086536\tAccuracy: 0.975400\n",
      "===========  n_hidden_units = 200\tepochs = 5\tl_rate = 0.2\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.230128\tEvaluation:\tLoss: 0.132218\tAccuracy: 0.959217\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.101494\tEvaluation:\tLoss: 0.087499\tAccuracy: 0.972700\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.069636\tEvaluation:\tLoss: 0.062970\tAccuracy: 0.979650\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.051246\tEvaluation:\tLoss: 0.048752\tAccuracy: 0.984000\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.038344\tEvaluation:\tLoss: 0.040647\tAccuracy: 0.986867\n",
      "Evaluation:\tLoss: 0.082675\tAccuracy: 0.975200\n",
      "===========  n_hidden_units = 200\tepochs = 5\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.481169\tEvaluation:\tLoss: 0.310666\tAccuracy: 0.909217\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.272183\tEvaluation:\tLoss: 0.245128\tAccuracy: 0.930383\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.223073\tEvaluation:\tLoss: 0.205285\tAccuracy: 0.942067\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.190680\tEvaluation:\tLoss: 0.177537\tAccuracy: 0.949783\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.167131\tEvaluation:\tLoss: 0.156663\tAccuracy: 0.955533\n",
      "Evaluation:\tLoss: 0.160490\tAccuracy: 0.953800\n",
      "===========  n_hidden_units = 200\tepochs = 5\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.645138\tEvaluation:\tLoss: 0.376597\tAccuracy: 0.894983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.335306\tEvaluation:\tLoss: 0.304957\tAccuracy: 0.914050\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.285208\tEvaluation:\tLoss: 0.267547\tAccuracy: 0.924283\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.253924\tEvaluation:\tLoss: 0.240717\tAccuracy: 0.932367\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.230113\tEvaluation:\tLoss: 0.219299\tAccuracy: 0.938500\n",
      "Evaluation:\tLoss: 0.214124\tAccuracy: 0.939500\n",
      "===========  n_hidden_units = 200\tepochs = 10\tl_rate = 0.2\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.195486\tEvaluation:\tLoss: 0.113273\tAccuracy: 0.964200\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.086662\tEvaluation:\tLoss: 0.086911\tAccuracy: 0.971083\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.057494\tEvaluation:\tLoss: 0.063052\tAccuracy: 0.979250\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.039881\tEvaluation:\tLoss: 0.041248\tAccuracy: 0.986250\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.028469\tEvaluation:\tLoss: 0.034779\tAccuracy: 0.988083\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.021597\tEvaluation:\tLoss: 0.033141\tAccuracy: 0.988667\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.016763\tEvaluation:\tLoss: 0.027524\tAccuracy: 0.990533\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.013203\tEvaluation:\tLoss: 0.024043\tAccuracy: 0.991467\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.011501\tEvaluation:\tLoss: 0.024834\tAccuracy: 0.991183\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.007697\tEvaluation:\tLoss: 0.020005\tAccuracy: 0.993067\n",
      "Evaluation:\tLoss: 0.104373\tAccuracy: 0.975500\n",
      "===========  n_hidden_units = 200\tepochs = 10\tl_rate = 0.2\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.228570\tEvaluation:\tLoss: 0.125803\tAccuracy: 0.960833\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.098568\tEvaluation:\tLoss: 0.081562\tAccuracy: 0.974767\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.067188\tEvaluation:\tLoss: 0.060680\tAccuracy: 0.980800\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.049878\tEvaluation:\tLoss: 0.047541\tAccuracy: 0.984833\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.037637\tEvaluation:\tLoss: 0.038903\tAccuracy: 0.987567\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.028452\tEvaluation:\tLoss: 0.030945\tAccuracy: 0.990467\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.021768\tEvaluation:\tLoss: 0.025387\tAccuracy: 0.992017\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.016734\tEvaluation:\tLoss: 0.022924\tAccuracy: 0.992617\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.012672\tEvaluation:\tLoss: 0.019658\tAccuracy: 0.993833\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.009906\tEvaluation:\tLoss: 0.018203\tAccuracy: 0.994383\n",
      "Evaluation:\tLoss: 0.081917\tAccuracy: 0.978400\n",
      "===========  n_hidden_units = 200\tepochs = 10\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.491086\tEvaluation:\tLoss: 0.312818\tAccuracy: 0.908783\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.273093\tEvaluation:\tLoss: 0.245998\tAccuracy: 0.929600\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.223907\tEvaluation:\tLoss: 0.206547\tAccuracy: 0.941317\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.191595\tEvaluation:\tLoss: 0.178558\tAccuracy: 0.949250\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.167740\tEvaluation:\tLoss: 0.157456\tAccuracy: 0.955033\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.149338\tEvaluation:\tLoss: 0.140915\tAccuracy: 0.960167\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.134643\tEvaluation:\tLoss: 0.127649\tAccuracy: 0.964183\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.122617\tEvaluation:\tLoss: 0.116769\tAccuracy: 0.967350\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.112522\tEvaluation:\tLoss: 0.107593\tAccuracy: 0.969867\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.103959\tEvaluation:\tLoss: 0.099750\tAccuracy: 0.972267\n",
      "Evaluation:\tLoss: 0.115393\tAccuracy: 0.967700\n",
      "===========  n_hidden_units = 200\tepochs = 10\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.626653\tEvaluation:\tLoss: 0.372488\tAccuracy: 0.896167\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.331239\tEvaluation:\tLoss: 0.302774\tAccuracy: 0.914533\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.282735\tEvaluation:\tLoss: 0.266367\tAccuracy: 0.925267\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.252498\tEvaluation:\tLoss: 0.240238\tAccuracy: 0.933217\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.229700\tEvaluation:\tLoss: 0.219755\tAccuracy: 0.939150\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.211338\tEvaluation:\tLoss: 0.202835\tAccuracy: 0.944000\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.195944\tEvaluation:\tLoss: 0.188475\tAccuracy: 0.947500\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.182682\tEvaluation:\tLoss: 0.175913\tAccuracy: 0.950633\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.171057\tEvaluation:\tLoss: 0.164913\tAccuracy: 0.953700\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.160873\tEvaluation:\tLoss: 0.155281\tAccuracy: 0.956517\n",
      "Evaluation:\tLoss: 0.162597\tAccuracy: 0.954800\n"
     ]
    }
   ],
   "source": [
    "for n_hidden_units in [100, 200]:\n",
    "    for epochs in [5, 10]:\n",
    "        for l_rate in [0.2, 0.01]:\n",
    "            for batch_size in [16, 32]:\n",
    "                print(f'===========  {n_hidden_units = }\\t{epochs = }\\t{l_rate = }\\t{batch_size = }  ===========')\n",
    "                prepare_and_train(n_hidden_units, epochs, l_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model for:\n",
    "* n_hidden_units = 200 \n",
    "* epochs = 10 \n",
    "* l_rate = 0.2\n",
    "* batch_size = 32 \n",
    "#### Accuracy = 0.9784."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Different optimziation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum:\n",
    "    def __init__(self, l_rate=0.01, momentum=0.9):\n",
    "        self.l_rate = l_rate\n",
    "        self.momentum = momentum\n",
    "        self.m = None\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            \n",
    "        self.m.W1 = self.momentum*self.m.W1 - self.l_rate*grads.W1\n",
    "        self.m.b1 = self.momentum*self.m.b1 - self.l_rate*grads.b1\n",
    "        self.m.W2 = self.momentum*self.m.W2 - self.l_rate*grads.W2\n",
    "        self.m.b2 = self.momentum*self.m.b2 - self.l_rate*grads.b2\n",
    "        \n",
    "        network.W1 += self.m.W1\n",
    "        network.b1 += self.m.b1\n",
    "        network.W2 += self.m.W2\n",
    "        network.b2 += self.m.b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, l_rate=0.01):\n",
    "        self.l_rate = l_rate\n",
    "        self.m = None\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            \n",
    "        self.m.W1 += grads.W1*grads.W1\n",
    "        self.m.b1 += grads.b1*grads.b1\n",
    "        self.m.W2 += grads.W2*grads.W2\n",
    "        self.m.b2 += grads.b2*grads.b2\n",
    "        \n",
    "        network.W1 -= self.l_rate*grads.W1 / np.sqrt(self.m.W1 + self.eps)\n",
    "        network.b1 -= self.l_rate*grads.b1 / np.sqrt(self.m.b1 + self.eps)\n",
    "        network.W2 -= self.l_rate*grads.W2 / np.sqrt(self.m.W2 + self.eps)\n",
    "        network.b2 -= self.l_rate*grads.b2 / np.sqrt(self.m.b2 + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self, l_rate=0.01, momentum=0.9):\n",
    "        self.l_rate = l_rate\n",
    "        self.momentum = momentum\n",
    "        self.m = None\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            \n",
    "        self.m.W1 = self.momentum*self.m.W1 + (1.0 - self.momentum)*grads.W1*grads.W1\n",
    "        self.m.b1 = self.momentum*self.m.b1 + (1.0 - self.momentum)*grads.b1*grads.b1\n",
    "        self.m.W2 = self.momentum*self.m.W2 + (1.0 - self.momentum)*grads.W2*grads.W2\n",
    "        self.m.b2 = self.momentum*self.m.b2 + (1.0 - self.momentum)*grads.b2*grads.b2\n",
    "        \n",
    "        network.W1 -= self.l_rate*grads.W1 / np.sqrt(self.m.W1 + self.eps)\n",
    "        network.b1 -= self.l_rate*grads.b1 / np.sqrt(self.m.b1 + self.eps)\n",
    "        network.W2 -= self.l_rate*grads.W2 / np.sqrt(self.m.W2 + self.eps)\n",
    "        network.b2 -= self.l_rate*grads.b2 / np.sqrt(self.m.b2 + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, l_rate=0.01, beta1=0.9, beta2=0.99):\n",
    "        self.l_rate = l_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = None\n",
    "        self.s = None\n",
    "        self.eps = 1e-10\n",
    "        self.t = 1\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            self.s = NetworkWeights(W1.copy(), b1.copy(), W2.copy(), b2.copy())\n",
    "            \n",
    "        self.m.W1 = self.beta1*self.m.W1 - (1.0 - self.beta1)*grads.W1\n",
    "        self.m.b1 = self.beta1*self.m.b1 - (1.0 - self.beta1)*grads.b1\n",
    "        self.m.W2 = self.beta1*self.m.W2 - (1.0 - self.beta1)*grads.W2\n",
    "        self.m.b2 = self.beta1*self.m.b2 - (1.0 - self.beta1)*grads.b2\n",
    "        \n",
    "        self.s.W1 = self.beta2*self.s.W1 + (1.0 - self.beta2)*grads.W1*grads.W1\n",
    "        self.s.b1 = self.beta2*self.s.b1 + (1.0 - self.beta2)*grads.b1*grads.b1\n",
    "        self.s.W2 = self.beta2*self.s.W2 + (1.0 - self.beta2)*grads.W2*grads.W2\n",
    "        self.s.b2 = self.beta2*self.s.b2 + (1.0 - self.beta2)*grads.b2*grads.b2\n",
    "        \n",
    "        mW1 = self.m.W1 / (1.0 - self.beta1**self.t)\n",
    "        mb1 = self.m.b1 / (1.0 - self.beta1**self.t)\n",
    "        mW2 = self.m.W2 / (1.0 - self.beta1**self.t)\n",
    "        mb2 = self.m.b2 / (1.0 - self.beta1**self.t)\n",
    "        \n",
    "        sW1 = self.s.W1 / (1.0 - self.beta2**self.t)\n",
    "        sb1 = self.s.b1 / (1.0 - self.beta2**self.t)\n",
    "        sW2 = self.s.W2 / (1.0 - self.beta2**self.t)\n",
    "        sb2 = self.s.b2 / (1.0 - self.beta2**self.t)\n",
    "        \n",
    "        network.W1 += self.l_rate*mW1 / np.sqrt(sW1 + self.eps)\n",
    "        network.b1 += self.l_rate*mb1 / np.sqrt(sb1 + self.eps)\n",
    "        network.W2 += self.l_rate*mW2 / np.sqrt(sW2 + self.eps)\n",
    "        network.b2 += self.l_rate*mb2 / np.sqrt(sb2 + self.eps)\n",
    "        \n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Testing optimziation algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(X, y, weights, optimizer):\n",
    "    grads, L = backward_pass(weights, X, y)\n",
    "    \n",
    "    optimizer.apply_grads(grads, weights)\n",
    "\n",
    "    return L\n",
    "\n",
    "def train(X, y, weights, optimizer, epochs, batch_size):\n",
    "    n_total = X.shape[0]\n",
    "    n_batches = n_total // batch_size + (1 if n_total % batch_size != 0 else 0)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        L = 0.0\n",
    "        for n_batch in range(1, n_batches+1):\n",
    "            batch_X = X[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            batch_y = y[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            L += train_one_batch(batch_X, batch_y, weights, optimizer)\n",
    "            print('\\rEpoch: {}\\tBatch: {}/{}\\tLoss: {:.6f}\\t'.format(epoch, n_batch, n_batches, L/n_batch), end='')\n",
    "        print('\\rEpoch: {}\\t'.format(epoch), end='')\n",
    "        evaluate(X, y, weights)\n",
    "\n",
    "def prepare_and_train(n_hidden_units, optimizer, epochs, batch_size):\n",
    "    weights = network_initialization(n_hidden_units)\n",
    "    train(X_train, y_train, weights, optimizer, epochs, batch_size)\n",
    "    evaluate(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.230360\tEvaluation:\tLoss: 0.143024\tAccuracy: 0.954900\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.101358\tEvaluation:\tLoss: 0.096667\tAccuracy: 0.968433\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.065895\tEvaluation:\tLoss: 0.065215\tAccuracy: 0.978433\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.046793\tEvaluation:\tLoss: 0.053981\tAccuracy: 0.981733\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.033830\tEvaluation:\tLoss: 0.040977\tAccuracy: 0.986133\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.024025\tEvaluation:\tLoss: 0.026080\tAccuracy: 0.991183\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.017561\tEvaluation:\tLoss: 0.023640\tAccuracy: 0.991767\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.013079\tEvaluation:\tLoss: 0.017215\tAccuracy: 0.994283\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.009483\tEvaluation:\tLoss: 0.016914\tAccuracy: 0.994367\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.006888\tEvaluation:\tLoss: 0.009679\tAccuracy: 0.997017\n",
      "Evaluation:\tLoss: 0.080080\tAccuracy: 0.978400\n"
     ]
    }
   ],
   "source": [
    "momentum = SGDMomentum(l_rate=0.03)\n",
    "prepare_and_train(200, momentum, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.192988\tEvaluation:\tLoss: 0.104383\tAccuracy: 0.969667\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.090914\tEvaluation:\tLoss: 0.071938\tAccuracy: 0.979250\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.066541\tEvaluation:\tLoss: 0.056337\tAccuracy: 0.983867\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.052779\tEvaluation:\tLoss: 0.046688\tAccuracy: 0.987083\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.043470\tEvaluation:\tLoss: 0.039510\tAccuracy: 0.989633\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.036671\tEvaluation:\tLoss: 0.034056\tAccuracy: 0.991367\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.031504\tEvaluation:\tLoss: 0.029889\tAccuracy: 0.992633\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.027388\tEvaluation:\tLoss: 0.026469\tAccuracy: 0.993933\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.024058\tEvaluation:\tLoss: 0.023542\tAccuracy: 0.994667\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.021332\tEvaluation:\tLoss: 0.021059\tAccuracy: 0.995550\n",
      "Evaluation:\tLoss: 0.067995\tAccuracy: 0.978800\n"
     ]
    }
   ],
   "source": [
    "adagrad = AdaGrad(l_rate=0.03)\n",
    "prepare_and_train(200, adagrad, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.194916\tEvaluation:\tLoss: 0.116074\tAccuracy: 0.965533\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.103709\tEvaluation:\tLoss: 0.100284\tAccuracy: 0.974183\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.081401\tEvaluation:\tLoss: 0.080509\tAccuracy: 0.979717\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.068313\tEvaluation:\tLoss: 0.077731\tAccuracy: 0.981850\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.057669\tEvaluation:\tLoss: 0.096365\tAccuracy: 0.978133\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.049626\tEvaluation:\tLoss: 0.070319\tAccuracy: 0.984517\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.049863\tEvaluation:\tLoss: 0.053614\tAccuracy: 0.988383\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.042183\tEvaluation:\tLoss: 0.055991\tAccuracy: 0.989150\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.033964\tEvaluation:\tLoss: 0.059295\tAccuracy: 0.988117\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.029653\tEvaluation:\tLoss: 0.049443\tAccuracy: 0.990033\n",
      "Evaluation:\tLoss: 0.231316\tAccuracy: 0.972100\n"
     ]
    }
   ],
   "source": [
    "rmsprop = RMSProp(l_rate=0.003)\n",
    "prepare_and_train(200, rmsprop, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.204840\tEvaluation:\tLoss: 0.181572\tAccuracy: 0.946433\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.095863\tEvaluation:\tLoss: 0.120989\tAccuracy: 0.963550\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.069214\tEvaluation:\tLoss: 0.077782\tAccuracy: 0.976650\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.055853\tEvaluation:\tLoss: 0.052027\tAccuracy: 0.983800\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.045032\tEvaluation:\tLoss: 0.049440\tAccuracy: 0.985700\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.036714\tEvaluation:\tLoss: 0.056177\tAccuracy: 0.984950\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.035412\tEvaluation:\tLoss: 0.039219\tAccuracy: 0.988917\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.029363\tEvaluation:\tLoss: 0.043676\tAccuracy: 0.987667\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.027796\tEvaluation:\tLoss: 0.027048\tAccuracy: 0.993033\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.026497\tEvaluation:\tLoss: 0.040849\tAccuracy: 0.990017\n",
      "Evaluation:\tLoss: 0.168664\tAccuracy: 0.977300\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(l_rate=0.003)\n",
    "prepare_and_train(200, adam, 10, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of advanced optimizer such as Momentum didn't help the model to achieve better results or faster convergence.\n",
    "Moreover RMSProp and Adam worsened previous results.\n",
    "The performance of the model improved with the use of AdaGrad optimization but only by 0.0004 of accurancy result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(weights, X, y, lambda_):\n",
    "    z1, a1, z2, a2 = forward_pass(weights, X)\n",
    "    \n",
    "    L = loss(a2, y.T) + lambda_/2.0 * (np.linalg.norm(weights.W1)**2 + np.linalg.norm(weights.W2)**2)\n",
    "    dL = np.matmul(d_softmax(z2).T, d_loss(a2, y.T)[np.newaxis, ...].T).T[0, ...]\n",
    "    grad_W2 = np.matmul(dL, a1.T) + lambda_*weights.W2\n",
    "    grad_b2 = np.mean(dL, axis=1, keepdims=True)\n",
    "    grad_W1 = np.matmul(np.matmul(weights.W2.T, dL) * D_relu(a1), X) + lambda_*weights.W1\n",
    "    grad_b1 = np.mean(np.matmul(weights.W2.T, dL) * D_relu(a1), axis=1, keepdims=True)  \n",
    "    return NetworkWeights(grad_W1, grad_b1, grad_W2, grad_b2), L\n",
    "\n",
    "def train_one_batch(X, y, weights, optimizer, lambda_):\n",
    "    grads, L = backward_pass(weights, X, y, lambda_)   \n",
    "    optimizer.apply_grads(grads, weights)\n",
    "    return L\n",
    "\n",
    "def train(X, y, weights, optimizer, epochs, batch_size, lambda_):\n",
    "    n_total = X.shape[0]\n",
    "    n_batches = n_total // batch_size + (1 if n_total % batch_size != 0 else 0)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        L = 0.0\n",
    "        for n_batch in range(1, n_batches+1):\n",
    "            batch_X = X[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            batch_y = y[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            L += train_one_batch(batch_X, batch_y, weights, optimizer, lambda_)\n",
    "            print('\\rEpoch: {}\\tBatch: {}/{}\\tLoss: {:.6f}\\t'.format(epoch, n_batch, n_batches, L/n_batch), end='')\n",
    "        print('\\rEpoch: {}\\t'.format(epoch), end='')\n",
    "        evaluate(X, y, weights)\n",
    "\n",
    "def prepare_and_train_2(n_hidden_units, optimizer, epochs, batch_size, lambda_):\n",
    "    weights = network_initialization(n_hidden_units)\n",
    "    train(X_train, y_train, weights, optimizer, epochs, batch_size, lambda_)\n",
    "    evaluate(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.637059\tEvaluation:\tLoss: 0.590668\tAccuracy: 0.848767\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.567517\tEvaluation:\tLoss: 0.469577\tAccuracy: 0.882400\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.546045\tEvaluation:\tLoss: 0.551237\tAccuracy: 0.839400\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.554611\tEvaluation:\tLoss: 0.510123\tAccuracy: 0.866933\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.556291\tEvaluation:\tLoss: 0.681219\tAccuracy: 0.818867\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.535381\tEvaluation:\tLoss: 0.572802\tAccuracy: 0.828133\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.536547\tEvaluation:\tLoss: 0.704173\tAccuracy: 0.809067\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.542912\tEvaluation:\tLoss: 0.532787\tAccuracy: 0.851467\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.539565\tEvaluation:\tLoss: 0.574547\tAccuracy: 0.841150\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.536208\tEvaluation:\tLoss: 0.597057\tAccuracy: 0.833717\n",
      "Evaluation:\tLoss: 0.589680\tAccuracy: 0.833800\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.882030\tEvaluation:\tLoss: 0.953037\tAccuracy: 0.758667\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.732805\tEvaluation:\tLoss: 0.918343\tAccuracy: 0.769050\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.729707\tEvaluation:\tLoss: 0.888063\tAccuracy: 0.765850\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.724452\tEvaluation:\tLoss: 0.795354\tAccuracy: 0.785100\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.716129\tEvaluation:\tLoss: 0.993366\tAccuracy: 0.752167\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.722921\tEvaluation:\tLoss: 0.787342\tAccuracy: 0.789650\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.720354\tEvaluation:\tLoss: 0.941778\tAccuracy: 0.755633\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.714601\tEvaluation:\tLoss: 0.924599\tAccuracy: 0.767767\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.721580\tEvaluation:\tLoss: 1.330005\tAccuracy: 0.733183\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.717008\tEvaluation:\tLoss: 1.000002\tAccuracy: 0.745583\n",
      "Evaluation:\tLoss: 1.008402\tAccuracy: 0.735900\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 1.347153\tEvaluation:\tLoss: 0.970253\tAccuracy: 0.679200\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 1.107573\tEvaluation:\tLoss: 0.911101\tAccuracy: 0.690917\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 1.107441\tEvaluation:\tLoss: 1.062144\tAccuracy: 0.653000\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 1.106942\tEvaluation:\tLoss: 1.109422\tAccuracy: 0.646050\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 1.105355\tEvaluation:\tLoss: 1.167978\tAccuracy: 0.620033\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 1.095845\tEvaluation:\tLoss: 1.192369\tAccuracy: 0.651983\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 1.088164\tEvaluation:\tLoss: 1.054334\tAccuracy: 0.665967\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 1.087505\tEvaluation:\tLoss: 1.119144\tAccuracy: 0.653500\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 1.085313\tEvaluation:\tLoss: 1.064898\tAccuracy: 0.671900\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 1.087384\tEvaluation:\tLoss: 1.317519\tAccuracy: 0.613217\n",
      "Evaluation:\tLoss: 1.350873\tAccuracy: 0.598600\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(l_rate=0.03)\n",
    "prepare_and_train_2(200, optimizer, 10, 32, 0.0002)\n",
    "prepare_and_train_2(200, optimizer, 10, 32, 0.002)\n",
    "prepare_and_train_2(200, optimizer, 10, 32, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was tested for various **hyper parameters**, namely\n",
    "* Number of epochs = [100,200]\n",
    "* Number of neurons in hidden layer = [5, 10]\n",
    "* Different learning rates = [0.2, 0.01]\n",
    "* Different minibatch sizes = [16, 32]\n",
    " \n",
    "The best result was obtained for\n",
    "* n_hidden_units = 200\n",
    "* epochs = 10\n",
    "* l_rate = 0.2\n",
    "* batch_size = 32\n",
    "#### with accurency = 0.9784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of advanced **optimizer** such as **Momentum** didn't help the model to achieve better results or faster convergence. \\\n",
    "Moreover **RMSProp and Adam** worsened previous results. \\\n",
    "The performance of the model improved with the use of\n",
    "**AdaGrad** optimization but only by 0.0004 of accurancy result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the **L2 regularizations** resulted in a model with a lower error rate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
