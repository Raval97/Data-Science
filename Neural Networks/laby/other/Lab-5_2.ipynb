{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXPUlLg7PHAF"
   },
   "source": [
    "**TASK:**\n",
    "\n",
    "Neural Networks and Deep Learning\n",
    "Cracow University of Technology\n",
    "\n",
    "Lab Assignment 5:\n",
    "\n",
    "The purpose of this laboratory is to implement a neural network for a classification task:\n",
    "\n",
    "\n",
    "\n",
    "1.   The network is trained using minibatch stochastic gradient descent.\n",
    "2.   You have images of handwritten digits from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and you should train the network to predict the value of the digit for images.\n",
    "\n",
    "Network specification:\n",
    "\n",
    "1.   Input layer - one hidden layer - output layer\n",
    "2.   Activation functions: for hidden layer \"ReLU\" and for output layer \"softmax\"\n",
    "3.   Loss function: categorical cross-entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gzip\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'mnist'\n",
    "\n",
    "test_images_path = os.path.join(dataset_path, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(dataset_path, 't10k-labels-idx1-ubyte.gz')\n",
    "train_images_path = os.path.join(dataset_path, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(dataset_path, 'train-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    if 'images' in path:\n",
    "        elem_size, header_bytes = 28, 16\n",
    "        type_ = np.float32\n",
    "    else:\n",
    "        elem_size, header_bytes = 1, 8\n",
    "        type_ = np.uint8\n",
    "        \n",
    "    if 't10k' in path:\n",
    "        num = 10000\n",
    "    else:\n",
    "        num = 60000\n",
    "        \n",
    "    f = gzip.open(path, 'r')\n",
    "    f.read(header_bytes)\n",
    "    shape = 1 if elem_size == 1 else (elem_size, elem_size)\n",
    "    \n",
    "    return np.array([\n",
    "        np.frombuffer(f.read(elem_size*elem_size), dtype=np.uint8).\n",
    "                      astype(type_).\n",
    "                      reshape(shape)\n",
    "        for _ in range(num)\n",
    "    ])\n",
    "\n",
    "def labels_to_one_hot(array):\n",
    "    n = array.shape[0]\n",
    "    res = np.zeros((n, 10))\n",
    "    res[np.arange(n), array.ravel()] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (60000, 784)\n",
      "y_train.shape = (60000, 10)\n",
      "X_test.shape = (10000, 784)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = read_dataset(train_images_path).reshape(-1, 784) / 255.\n",
    "y_train = labels_to_one_hot(read_dataset(train_labels_path))\n",
    "X_test = read_dataset(test_images_path).reshape(-1, 784) / 255.\n",
    "y_test = labels_to_one_hot(read_dataset(test_labels_path))\n",
    "\n",
    "print(f'{X_train.shape = }')\n",
    "print(f'{y_train.shape = }')\n",
    "print(f'{X_test.shape = }')\n",
    "print(f'{y_test.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s8Mq_ZnWWVzU"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_ = np.exp(x)\n",
    "    return exp_ / np.sum(exp_, axis=0)\n",
    "\n",
    "def d_softmax(x):\n",
    "    y = softmax(x)\n",
    "    n = y.shape[1]\n",
    "    tiled = np.tile(y, (10, 1, 1))\n",
    "    return tiled * (np.diag([1]*10)[..., np.newaxis]-tiled.transpose(1, 0, 2))\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x > 0.0, x, 0.0)\n",
    "\n",
    "def D_relu(x):\n",
    "    return np.where(x > 0.0, 1.0, 0.0)\n",
    "\n",
    "def loss(predicted, target):\n",
    "    return -np.mean(np.sum(target*np.log(predicted), axis=0))\n",
    "\n",
    "def d_loss(predicted, target):\n",
    "    return - target / predicted / predicted.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7FFYLpnf6oT"
   },
   "source": [
    "**TASK:**\n",
    "\n",
    "Your code consists of at least five functions:\n",
    "\n",
    "* Network initialization\n",
    "* Forward pass\n",
    "* Backward pass\n",
    "* Train \n",
    "* Evaluate\n",
    "\n",
    "You are free to add more functions for the sake of having better organization for your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkWeights:\n",
    "    def __init__(self, W1, b1, W2, b2):\n",
    "        self.W1 = W1\n",
    "        self.b1 = b1\n",
    "        self.W2 = W2\n",
    "        self.b2 = b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_hidden_units):\n",
    "    W1 = np.random.normal(loc=0.0,\n",
    "                          scale=np.sqrt(2.0/784),\n",
    "                          size=(n_hidden_units, 784))\n",
    "    b1 = np.zeros((n_hidden_units, 1))\n",
    "    W2 = np.random.normal(loc=0.0,\n",
    "                          scale=np.sqrt(2.0/(n_hidden_units+10)),\n",
    "                          size=(10, n_hidden_units))\n",
    "    b2 = np.zeros((10, 1))\n",
    "    \n",
    "    return NetworkWeights(W1, b1, W2, b2)\n",
    "\n",
    "\n",
    "def forward_pass(weights, X):\n",
    "    z1 = np.matmul(weights.W1, X.T) + weights.b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.matmul(weights.W2, a1) + weights.b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "\n",
    "def backward_pass(weights, X, y):\n",
    "    z1, a1, z2, a2 = forward_pass(weights, X)\n",
    "    \n",
    "    L = loss(a2, y.T)\n",
    "    dL = np.matmul(d_softmax(z2).T, d_loss(a2, y.T)[np.newaxis, ...].T).T[0, ...]\n",
    "    grad_W2 = np.matmul(dL, a1.T)\n",
    "    grad_b2 = np.mean(dL, axis=1, keepdims=True)\n",
    "    grad_W1 = np.matmul(np.matmul(weights.W2.T, dL) * D_relu(a1), X)\n",
    "    grad_b1 = np.mean(np.matmul(weights.W2.T, dL) * D_relu(a1), axis=1, keepdims=True)\n",
    "    \n",
    "    return NetworkWeights(grad_W1, grad_b1, grad_W2, grad_b2), L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y, weights):\n",
    "    _, _, _, a2 = forward_pass(weights, X)\n",
    "    L = loss(a2, y.T)\n",
    "    a2_ = np.argmax(a2.T, axis=1)\n",
    "    y_ = np.argmax(y, axis=1)\n",
    "    print(f'Evaluation:\\tLoss: {L:.6f}\\tAccuracy: {np.mean(a2_ == y_):.6f}')\n",
    "    \n",
    "\n",
    "def train_one_batch(X, y, weights, l_rate):\n",
    "    grads, L = backward_pass(weights, X, y)\n",
    "    \n",
    "    weights.W2 -= grads.W2 * l_rate\n",
    "    weights.b2 -= grads.b2 * l_rate\n",
    "    weights.W1 -= grads.W1 * l_rate\n",
    "    weights.b1 -= grads.b1 * l_rate\n",
    "\n",
    "    return L\n",
    "\n",
    "def train(X, y, weights, l_rate, epochs, batch_size):\n",
    "    n_total = X.shape[0]\n",
    "    n_batches = n_total // batch_size + (1 if n_total % batch_size != 0 else 0)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        L = 0.0\n",
    "        for n_batch in range(1, n_batches+1):\n",
    "            batch_X = X[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            batch_y = y[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            L += train_one_batch(batch_X, batch_y, weights, l_rate)\n",
    "            print('\\rEpoch: {}\\tBatch: {}/{}\\tLoss: {:.6f}\\t'.format(epoch, n_batch, n_batches, L/n_batch), end='')\n",
    "        print('\\rEpoch: {}\\t'.format(epoch), end='')\n",
    "        evaluate(X, y, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7VByNZMXsNa"
   },
   "source": [
    "**TASK:**\n",
    "\n",
    "Tune your network by changing hyperparametes of the network:\n",
    "* Number of epochs\n",
    "* Number of neurons in hidden layer\n",
    "* Different learning rates\n",
    "* Different minibatch sizes\n",
    "\n",
    "Also, try the following changes to the network:\n",
    "* Apply different optimziation algorithms: Momentum, Adagrad, RMSprop, and ADAM\n",
    "* Apply L2 regularization techniques to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_train(n_hidden_units, epochs, l_rate, batch_size):\n",
    "    weights = initialize_network(n_hidden_units)\n",
    "    train(X_train, y_train, weights, l_rate, epochs, batch_size)\n",
    "    evaluate(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========  n_hidden_units = 150\tepochs = 5\tl_rate = 0.3\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.204243\tEvaluation:\tLoss: 0.117537\tAccuracy: 0.962967\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.099515\tEvaluation:\tLoss: 0.087846\tAccuracy: 0.971400\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.069591\tEvaluation:\tLoss: 0.083425\tAccuracy: 0.972850\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.055790\tEvaluation:\tLoss: 0.059070\tAccuracy: 0.981467\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.044740\tEvaluation:\tLoss: 0.059667\tAccuracy: 0.980950\n",
      "Evaluation:\tLoss: 0.128008\tAccuracy: 0.966800\n",
      "===========  n_hidden_units = 150\tepochs = 5\tl_rate = 0.3\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.214377\tEvaluation:\tLoss: 0.119084\tAccuracy: 0.963750\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.091966\tEvaluation:\tLoss: 0.072200\tAccuracy: 0.977733\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.062882\tEvaluation:\tLoss: 0.057859\tAccuracy: 0.980967\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.045759\tEvaluation:\tLoss: 0.043199\tAccuracy: 0.986417\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.032834\tEvaluation:\tLoss: 0.037282\tAccuracy: 0.987433\n",
      "Evaluation:\tLoss: 0.087663\tAccuracy: 0.975300\n",
      "===========  n_hidden_units = 150\tepochs = 5\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.502905\tEvaluation:\tLoss: 0.316680\tAccuracy: 0.907150\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.277748\tEvaluation:\tLoss: 0.253195\tAccuracy: 0.926650\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.229981\tEvaluation:\tLoss: 0.213908\tAccuracy: 0.938617\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.197523\tEvaluation:\tLoss: 0.185443\tAccuracy: 0.946917\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.173522\tEvaluation:\tLoss: 0.164024\tAccuracy: 0.953517\n",
      "Evaluation:\tLoss: 0.168444\tAccuracy: 0.951600\n",
      "===========  n_hidden_units = 150\tepochs = 5\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.650812\tEvaluation:\tLoss: 0.373680\tAccuracy: 0.896000\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.332583\tEvaluation:\tLoss: 0.302929\tAccuracy: 0.914350\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.283957\tEvaluation:\tLoss: 0.266298\tAccuracy: 0.924567\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.253356\tEvaluation:\tLoss: 0.239542\tAccuracy: 0.932450\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.229842\tEvaluation:\tLoss: 0.218314\tAccuracy: 0.938433\n",
      "Evaluation:\tLoss: 0.214828\tAccuracy: 0.939400\n",
      "===========  n_hidden_units = 150\tepochs = 15\tl_rate = 0.3\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.204090\tEvaluation:\tLoss: 0.124290\tAccuracy: 0.961100\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.101601\tEvaluation:\tLoss: 0.089592\tAccuracy: 0.971417\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.073244\tEvaluation:\tLoss: 0.079522\tAccuracy: 0.975483\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.053778\tEvaluation:\tLoss: 0.047208\tAccuracy: 0.984517\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.045129\tEvaluation:\tLoss: 0.049089\tAccuracy: 0.984367\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.039161\tEvaluation:\tLoss: 0.066097\tAccuracy: 0.980000\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.035683\tEvaluation:\tLoss: 0.054985\tAccuracy: 0.982967\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.031148\tEvaluation:\tLoss: 0.032927\tAccuracy: 0.989100\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.028149\tEvaluation:\tLoss: 0.031022\tAccuracy: 0.989967\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.024910\tEvaluation:\tLoss: 0.029347\tAccuracy: 0.990417\n",
      "Epoch: 11\tBatch: 3750/3750\tLoss: 0.022206\tEvaluation:\tLoss: 0.029306\tAccuracy: 0.990500\n",
      "Epoch: 12\tBatch: 3750/3750\tLoss: 0.019782\tEvaluation:\tLoss: 0.026143\tAccuracy: 0.992100\n",
      "Epoch: 13\tBatch: 3750/3750\tLoss: 0.024098\tEvaluation:\tLoss: 0.034358\tAccuracy: 0.989317\n",
      "Epoch: 14\tBatch: 3750/3750\tLoss: 0.020471\tEvaluation:\tLoss: 0.029803\tAccuracy: 0.990817\n",
      "Epoch: 15\tBatch: 3750/3750\tLoss: 0.017200\tEvaluation:\tLoss: 0.027061\tAccuracy: 0.991817\n",
      "Evaluation:\tLoss: 0.156862\tAccuracy: 0.974800\n",
      "===========  n_hidden_units = 150\tepochs = 15\tl_rate = 0.3\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.219441\tEvaluation:\tLoss: 0.119382\tAccuracy: 0.962633\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.093651\tEvaluation:\tLoss: 0.080506\tAccuracy: 0.973883\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.063605\tEvaluation:\tLoss: 0.057569\tAccuracy: 0.981117\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.046328\tEvaluation:\tLoss: 0.043930\tAccuracy: 0.985833\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.034095\tEvaluation:\tLoss: 0.035686\tAccuracy: 0.988567\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.025244\tEvaluation:\tLoss: 0.028756\tAccuracy: 0.990517\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.019394\tEvaluation:\tLoss: 0.024433\tAccuracy: 0.992000\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.014287\tEvaluation:\tLoss: 0.020489\tAccuracy: 0.992883\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.010700\tEvaluation:\tLoss: 0.018497\tAccuracy: 0.993933\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.008138\tEvaluation:\tLoss: 0.015680\tAccuracy: 0.994917\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.006122\tEvaluation:\tLoss: 0.014084\tAccuracy: 0.995350\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.004330\tEvaluation:\tLoss: 0.010660\tAccuracy: 0.996650\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.003200\tEvaluation:\tLoss: 0.006489\tAccuracy: 0.998317\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.002488\tEvaluation:\tLoss: 0.004576\tAccuracy: 0.999100\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.001990\tEvaluation:\tLoss: 0.004058\tAccuracy: 0.999267\n",
      "Evaluation:\tLoss: 0.083718\tAccuracy: 0.979700\n",
      "===========  n_hidden_units = 150\tepochs = 15\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.486711\tEvaluation:\tLoss: 0.313716\tAccuracy: 0.908033\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.275035\tEvaluation:\tLoss: 0.250099\tAccuracy: 0.928700\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.228375\tEvaluation:\tLoss: 0.212020\tAccuracy: 0.939783\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.197126\tEvaluation:\tLoss: 0.184846\tAccuracy: 0.947900\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.173529\tEvaluation:\tLoss: 0.163599\tAccuracy: 0.954333\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.154723\tEvaluation:\tLoss: 0.146610\tAccuracy: 0.959167\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.139542\tEvaluation:\tLoss: 0.132902\tAccuracy: 0.963300\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.127061\tEvaluation:\tLoss: 0.121600\tAccuracy: 0.966467\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.116513\tEvaluation:\tLoss: 0.111997\tAccuracy: 0.969083\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.107563\tEvaluation:\tLoss: 0.103862\tAccuracy: 0.971300\n",
      "Epoch: 11\tBatch: 3750/3750\tLoss: 0.099846\tEvaluation:\tLoss: 0.096816\tAccuracy: 0.973067\n",
      "Epoch: 12\tBatch: 3750/3750\tLoss: 0.093133\tEvaluation:\tLoss: 0.090589\tAccuracy: 0.975067\n",
      "Epoch: 13\tBatch: 3750/3750\tLoss: 0.087199\tEvaluation:\tLoss: 0.085125\tAccuracy: 0.976700\n",
      "Epoch: 14\tBatch: 3750/3750\tLoss: 0.081968\tEvaluation:\tLoss: 0.080151\tAccuracy: 0.978267\n",
      "Epoch: 15\tBatch: 3750/3750\tLoss: 0.077283\tEvaluation:\tLoss: 0.075748\tAccuracy: 0.979533\n",
      "Evaluation:\tLoss: 0.094047\tAccuracy: 0.972600\n",
      "===========  n_hidden_units = 150\tepochs = 15\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.648957\tEvaluation:\tLoss: 0.375713\tAccuracy: 0.894417\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.336183\tEvaluation:\tLoss: 0.305833\tAccuracy: 0.913800\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.287869\tEvaluation:\tLoss: 0.269468\tAccuracy: 0.924500\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.257726\tEvaluation:\tLoss: 0.243661\tAccuracy: 0.932100\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.234854\tEvaluation:\tLoss: 0.223098\tAccuracy: 0.937950\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.216190\tEvaluation:\tLoss: 0.206032\tAccuracy: 0.942650\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.200514\tEvaluation:\tLoss: 0.191515\tAccuracy: 0.946650\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.187049\tEvaluation:\tLoss: 0.178978\tAccuracy: 0.949883\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.175307\tEvaluation:\tLoss: 0.168017\tAccuracy: 0.952983\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.164997\tEvaluation:\tLoss: 0.158365\tAccuracy: 0.955900\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.155901\tEvaluation:\tLoss: 0.149778\tAccuracy: 0.958350\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.147730\tEvaluation:\tLoss: 0.142097\tAccuracy: 0.960683\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.140399\tEvaluation:\tLoss: 0.135183\tAccuracy: 0.962750\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.133786\tEvaluation:\tLoss: 0.128994\tAccuracy: 0.964483\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.127816\tEvaluation:\tLoss: 0.123374\tAccuracy: 0.966200\n",
      "Evaluation:\tLoss: 0.133059\tAccuracy: 0.961300\n",
      "===========  n_hidden_units = 300\tepochs = 5\tl_rate = 0.3\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.191445\tEvaluation:\tLoss: 0.121332\tAccuracy: 0.961683\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.086608\tEvaluation:\tLoss: 0.084336\tAccuracy: 0.972683\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.057248\tEvaluation:\tLoss: 0.051904\tAccuracy: 0.982800\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.042347\tEvaluation:\tLoss: 0.046307\tAccuracy: 0.985217\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.031711\tEvaluation:\tLoss: 0.036568\tAccuracy: 0.987467\n",
      "Evaluation:\tLoss: 0.098449\tAccuracy: 0.973800\n",
      "===========  n_hidden_units = 300\tepochs = 5\tl_rate = 0.3\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.206641\tEvaluation:\tLoss: 0.107823\tAccuracy: 0.966583\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.085347\tEvaluation:\tLoss: 0.066348\tAccuracy: 0.979217\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.054832\tEvaluation:\tLoss: 0.048294\tAccuracy: 0.984217\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.037401\tEvaluation:\tLoss: 0.037516\tAccuracy: 0.987850\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.025190\tEvaluation:\tLoss: 0.029358\tAccuracy: 0.990350\n",
      "Evaluation:\tLoss: 0.075647\tAccuracy: 0.976800\n",
      "===========  n_hidden_units = 300\tepochs = 5\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.480294\tEvaluation:\tLoss: 0.305739\tAccuracy: 0.911867\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.266886\tEvaluation:\tLoss: 0.239663\tAccuracy: 0.931500\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.217390\tEvaluation:\tLoss: 0.200052\tAccuracy: 0.943817\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.184506\tEvaluation:\tLoss: 0.172111\tAccuracy: 0.951667\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.160627\tEvaluation:\tLoss: 0.151032\tAccuracy: 0.957683\n",
      "Evaluation:\tLoss: 0.155312\tAccuracy: 0.955200\n",
      "===========  n_hidden_units = 300\tepochs = 5\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.631602\tEvaluation:\tLoss: 0.374366\tAccuracy: 0.895217\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.332657\tEvaluation:\tLoss: 0.302704\tAccuracy: 0.914133\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.282544\tEvaluation:\tLoss: 0.264804\tAccuracy: 0.925100\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.250992\tEvaluation:\tLoss: 0.237618\tAccuracy: 0.933217\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.227024\tEvaluation:\tLoss: 0.216151\tAccuracy: 0.939167\n",
      "Evaluation:\tLoss: 0.213893\tAccuracy: 0.940400\n",
      "===========  n_hidden_units = 300\tepochs = 15\tl_rate = 0.3\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.193164\tEvaluation:\tLoss: 0.099790\tAccuracy: 0.968750\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.084956\tEvaluation:\tLoss: 0.084412\tAccuracy: 0.973333\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.055075\tEvaluation:\tLoss: 0.049726\tAccuracy: 0.983750\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.039039\tEvaluation:\tLoss: 0.040976\tAccuracy: 0.986833\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.033317\tEvaluation:\tLoss: 0.035256\tAccuracy: 0.987950\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.024754\tEvaluation:\tLoss: 0.024758\tAccuracy: 0.991617\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.023973\tEvaluation:\tLoss: 0.027458\tAccuracy: 0.990700\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.017512\tEvaluation:\tLoss: 0.025058\tAccuracy: 0.991633\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.017341\tEvaluation:\tLoss: 0.027437\tAccuracy: 0.990817\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.016755\tEvaluation:\tLoss: 0.022965\tAccuracy: 0.992467\n",
      "Epoch: 11\tBatch: 3750/3750\tLoss: 0.016039\tEvaluation:\tLoss: 0.025311\tAccuracy: 0.991867\n",
      "Epoch: 12\tBatch: 3750/3750\tLoss: 0.013105\tEvaluation:\tLoss: 0.036792\tAccuracy: 0.989367\n",
      "Epoch: 13\tBatch: 3750/3750\tLoss: 0.010835\tEvaluation:\tLoss: 0.013947\tAccuracy: 0.995150\n",
      "Epoch: 14\tBatch: 3750/3750\tLoss: 0.009859\tEvaluation:\tLoss: 0.010190\tAccuracy: 0.996450\n",
      "Epoch: 15\tBatch: 3750/3750\tLoss: 0.006291\tEvaluation:\tLoss: 0.008981\tAccuracy: 0.996833\n",
      "Evaluation:\tLoss: 0.110434\tAccuracy: 0.979300\n",
      "===========  n_hidden_units = 300\tepochs = 15\tl_rate = 0.3\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.205250\tEvaluation:\tLoss: 0.103291\tAccuracy: 0.967900\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.083697\tEvaluation:\tLoss: 0.066704\tAccuracy: 0.978367\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.054246\tEvaluation:\tLoss: 0.046734\tAccuracy: 0.984900\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.036551\tEvaluation:\tLoss: 0.036231\tAccuracy: 0.988750\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.024796\tEvaluation:\tLoss: 0.032907\tAccuracy: 0.989167\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.016563\tEvaluation:\tLoss: 0.023621\tAccuracy: 0.992367\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.011458\tEvaluation:\tLoss: 0.020824\tAccuracy: 0.993283\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.008247\tEvaluation:\tLoss: 0.015591\tAccuracy: 0.995033\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.005764\tEvaluation:\tLoss: 0.012967\tAccuracy: 0.996317\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.004041\tEvaluation:\tLoss: 0.009810\tAccuracy: 0.997300\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.003062\tEvaluation:\tLoss: 0.005862\tAccuracy: 0.998900\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.002370\tEvaluation:\tLoss: 0.004304\tAccuracy: 0.999283\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.001942\tEvaluation:\tLoss: 0.003550\tAccuracy: 0.999483\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.001644\tEvaluation:\tLoss: 0.002927\tAccuracy: 0.999683\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.001427\tEvaluation:\tLoss: 0.002487\tAccuracy: 0.999817\n",
      "Evaluation:\tLoss: 0.071493\tAccuracy: 0.981800\n",
      "===========  n_hidden_units = 300\tepochs = 15\tl_rate = 0.01\tbatch_size = 16  ===========\n",
      "Epoch: 1\tBatch: 3750/3750\tLoss: 0.472849\tEvaluation:\tLoss: 0.305460\tAccuracy: 0.911867\n",
      "Epoch: 2\tBatch: 3750/3750\tLoss: 0.266881\tEvaluation:\tLoss: 0.240724\tAccuracy: 0.931317\n",
      "Epoch: 3\tBatch: 3750/3750\tLoss: 0.218245\tEvaluation:\tLoss: 0.201062\tAccuracy: 0.943100\n",
      "Epoch: 4\tBatch: 3750/3750\tLoss: 0.185626\tEvaluation:\tLoss: 0.172660\tAccuracy: 0.951200\n",
      "Epoch: 5\tBatch: 3750/3750\tLoss: 0.161579\tEvaluation:\tLoss: 0.151402\tAccuracy: 0.957200\n",
      "Epoch: 6\tBatch: 3750/3750\tLoss: 0.143110\tEvaluation:\tLoss: 0.134710\tAccuracy: 0.962650\n",
      "Epoch: 7\tBatch: 3750/3750\tLoss: 0.128444\tEvaluation:\tLoss: 0.121477\tAccuracy: 0.966467\n",
      "Epoch: 8\tBatch: 3750/3750\tLoss: 0.116529\tEvaluation:\tLoss: 0.110558\tAccuracy: 0.969683\n",
      "Epoch: 9\tBatch: 3750/3750\tLoss: 0.106616\tEvaluation:\tLoss: 0.101394\tAccuracy: 0.972183\n",
      "Epoch: 10\tBatch: 3750/3750\tLoss: 0.098193\tEvaluation:\tLoss: 0.093604\tAccuracy: 0.974417\n",
      "Epoch: 11\tBatch: 3750/3750\tLoss: 0.090953\tEvaluation:\tLoss: 0.086870\tAccuracy: 0.976233\n",
      "Epoch: 12\tBatch: 3750/3750\tLoss: 0.084621\tEvaluation:\tLoss: 0.081031\tAccuracy: 0.977983\n",
      "Epoch: 13\tBatch: 3750/3750\tLoss: 0.079046\tEvaluation:\tLoss: 0.075850\tAccuracy: 0.979600\n",
      "Epoch: 14\tBatch: 3750/3750\tLoss: 0.074103\tEvaluation:\tLoss: 0.071230\tAccuracy: 0.980733\n",
      "Epoch: 15\tBatch: 3750/3750\tLoss: 0.069674\tEvaluation:\tLoss: 0.067095\tAccuracy: 0.982033\n",
      "Evaluation:\tLoss: 0.088035\tAccuracy: 0.973900\n",
      "===========  n_hidden_units = 300\tepochs = 15\tl_rate = 0.01\tbatch_size = 32  ===========\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.616749\tEvaluation:\tLoss: 0.370292\tAccuracy: 0.896067\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.329532\tEvaluation:\tLoss: 0.300558\tAccuracy: 0.915183\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.280266\tEvaluation:\tLoss: 0.262903\tAccuracy: 0.926350\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.249083\tEvaluation:\tLoss: 0.236075\tAccuracy: 0.934317\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.225500\tEvaluation:\tLoss: 0.214831\tAccuracy: 0.940500\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.206348\tEvaluation:\tLoss: 0.197249\tAccuracy: 0.945183\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.190158\tEvaluation:\tLoss: 0.182192\tAccuracy: 0.949217\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.176265\tEvaluation:\tLoss: 0.169245\tAccuracy: 0.953317\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.164252\tEvaluation:\tLoss: 0.158084\tAccuracy: 0.956300\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.153807\tEvaluation:\tLoss: 0.148329\tAccuracy: 0.959083\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.144614\tEvaluation:\tLoss: 0.139701\tAccuracy: 0.961217\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.136468\tEvaluation:\tLoss: 0.131999\tAccuracy: 0.963183\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.129152\tEvaluation:\tLoss: 0.125062\tAccuracy: 0.965017\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.122558\tEvaluation:\tLoss: 0.118832\tAccuracy: 0.966900\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.116596\tEvaluation:\tLoss: 0.113179\tAccuracy: 0.968783\n",
      "Evaluation:\tLoss: 0.125145\tAccuracy: 0.963900\n"
     ]
    }
   ],
   "source": [
    "for n_hidden_units in [150, 300]:\n",
    "    for epochs in [5, 15]:\n",
    "        for l_rate in [0.3, 0.01]:\n",
    "            for batch_size in [16, 32]:\n",
    "                print(f'===========  {n_hidden_units = }\\t{epochs = }\\t{l_rate = }\\t{batch_size = }  ===========')\n",
    "                prepare_and_train(n_hidden_units, epochs, l_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model so far: n_hidden_units = 300, epochs = 15, l_rate = 0.3, batch_size = 32; Accuracy: 0.981800."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum:\n",
    "    def __init__(self, l_rate=0.01, momentum=0.9):\n",
    "        self.l_rate = l_rate\n",
    "        self.momentum = momentum\n",
    "        self.m = None\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            \n",
    "        self.m.W1 = self.momentum*self.m.W1 - self.l_rate*grads.W1\n",
    "        self.m.b1 = self.momentum*self.m.b1 - self.l_rate*grads.b1\n",
    "        self.m.W2 = self.momentum*self.m.W2 - self.l_rate*grads.W2\n",
    "        self.m.b2 = self.momentum*self.m.b2 - self.l_rate*grads.b2\n",
    "        \n",
    "        network.W1 += self.m.W1\n",
    "        network.b1 += self.m.b1\n",
    "        network.W2 += self.m.W2\n",
    "        network.b2 += self.m.b2\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, l_rate=0.01):\n",
    "        self.l_rate = l_rate\n",
    "        self.m = None\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            \n",
    "        self.m.W1 += grads.W1*grads.W1\n",
    "        self.m.b1 += grads.b1*grads.b1\n",
    "        self.m.W2 += grads.W2*grads.W2\n",
    "        self.m.b2 += grads.b2*grads.b2\n",
    "        \n",
    "        network.W1 -= self.l_rate*grads.W1 / np.sqrt(self.m.W1 + self.eps)\n",
    "        network.b1 -= self.l_rate*grads.b1 / np.sqrt(self.m.b1 + self.eps)\n",
    "        network.W2 -= self.l_rate*grads.W2 / np.sqrt(self.m.W2 + self.eps)\n",
    "        network.b2 -= self.l_rate*grads.b2 / np.sqrt(self.m.b2 + self.eps)\n",
    "\n",
    "\n",
    "class RMSProp:\n",
    "    def __init__(self, l_rate=0.01, momentum=0.9):\n",
    "        self.l_rate = l_rate\n",
    "        self.momentum = momentum\n",
    "        self.m = None\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            \n",
    "        self.m.W1 = self.momentum*self.m.W1 + (1.0 - self.momentum)*grads.W1*grads.W1\n",
    "        self.m.b1 = self.momentum*self.m.b1 + (1.0 - self.momentum)*grads.b1*grads.b1\n",
    "        self.m.W2 = self.momentum*self.m.W2 + (1.0 - self.momentum)*grads.W2*grads.W2\n",
    "        self.m.b2 = self.momentum*self.m.b2 + (1.0 - self.momentum)*grads.b2*grads.b2\n",
    "        \n",
    "        network.W1 -= self.l_rate*grads.W1 / np.sqrt(self.m.W1 + self.eps)\n",
    "        network.b1 -= self.l_rate*grads.b1 / np.sqrt(self.m.b1 + self.eps)\n",
    "        network.W2 -= self.l_rate*grads.W2 / np.sqrt(self.m.W2 + self.eps)\n",
    "        network.b2 -= self.l_rate*grads.b2 / np.sqrt(self.m.b2 + self.eps)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, l_rate=0.01, beta1=0.9, beta2=0.99):\n",
    "        self.l_rate = l_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = None\n",
    "        self.s = None\n",
    "        self.eps = 1e-10\n",
    "        self.t = 1\n",
    "    \n",
    "    def apply_grads(self, grads, network):\n",
    "        if self.m is None:\n",
    "            W1 = np.zeros_like(grads.W1)\n",
    "            b1 = np.zeros_like(grads.b1)\n",
    "            W2 = np.zeros_like(grads.W2)\n",
    "            b2 = np.zeros_like(grads.b2)\n",
    "            self.m = NetworkWeights(W1, b1, W2, b2)\n",
    "            self.s = NetworkWeights(W1.copy(), b1.copy(), W2.copy(), b2.copy())\n",
    "            \n",
    "        self.m.W1 = self.beta1*self.m.W1 - (1.0 - self.beta1)*grads.W1\n",
    "        self.m.b1 = self.beta1*self.m.b1 - (1.0 - self.beta1)*grads.b1\n",
    "        self.m.W2 = self.beta1*self.m.W2 - (1.0 - self.beta1)*grads.W2\n",
    "        self.m.b2 = self.beta1*self.m.b2 - (1.0 - self.beta1)*grads.b2\n",
    "        \n",
    "        self.s.W1 = self.beta2*self.s.W1 + (1.0 - self.beta2)*grads.W1*grads.W1\n",
    "        self.s.b1 = self.beta2*self.s.b1 + (1.0 - self.beta2)*grads.b1*grads.b1\n",
    "        self.s.W2 = self.beta2*self.s.W2 + (1.0 - self.beta2)*grads.W2*grads.W2\n",
    "        self.s.b2 = self.beta2*self.s.b2 + (1.0 - self.beta2)*grads.b2*grads.b2\n",
    "        \n",
    "        mW1 = self.m.W1 / (1.0 - self.beta1**self.t)\n",
    "        mb1 = self.m.b1 / (1.0 - self.beta1**self.t)\n",
    "        mW2 = self.m.W2 / (1.0 - self.beta1**self.t)\n",
    "        mb2 = self.m.b2 / (1.0 - self.beta1**self.t)\n",
    "        \n",
    "        sW1 = self.s.W1 / (1.0 - self.beta2**self.t)\n",
    "        sb1 = self.s.b1 / (1.0 - self.beta2**self.t)\n",
    "        sW2 = self.s.W2 / (1.0 - self.beta2**self.t)\n",
    "        sb2 = self.s.b2 / (1.0 - self.beta2**self.t)\n",
    "        \n",
    "        network.W1 += self.l_rate*mW1 / np.sqrt(sW1 + self.eps)\n",
    "        network.b1 += self.l_rate*mb1 / np.sqrt(sb1 + self.eps)\n",
    "        network.W2 += self.l_rate*mW2 / np.sqrt(sW2 + self.eps)\n",
    "        network.b2 += self.l_rate*mb2 / np.sqrt(sb2 + self.eps)\n",
    "        \n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(X, y, weights, optimizer):\n",
    "    grads, L = backward_pass(weights, X, y)\n",
    "    \n",
    "    optimizer.apply_grads(grads, weights)\n",
    "\n",
    "    return L\n",
    "\n",
    "def train(X, y, weights, optimizer, epochs, batch_size):\n",
    "    n_total = X.shape[0]\n",
    "    n_batches = n_total // batch_size + (1 if n_total % batch_size != 0 else 0)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        L = 0.0\n",
    "        for n_batch in range(1, n_batches+1):\n",
    "            batch_X = X[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            batch_y = y[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            L += train_one_batch(batch_X, batch_y, weights, optimizer)\n",
    "            print('\\rEpoch: {}\\tBatch: {}/{}\\tLoss: {:.6f}\\t'.format(epoch, n_batch, n_batches, L/n_batch), end='')\n",
    "        print('\\rEpoch: {}\\t'.format(epoch), end='')\n",
    "        evaluate(X, y, weights)\n",
    "\n",
    "def prepare_and_train(n_hidden_units, optimizer, epochs, batch_size):\n",
    "    weights = initialize_network(n_hidden_units)\n",
    "    train(X_train, y_train, weights, optimizer, epochs, batch_size)\n",
    "    evaluate(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.223107\tEvaluation:\tLoss: 0.140579\tAccuracy: 0.956350\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.095570\tEvaluation:\tLoss: 0.084861\tAccuracy: 0.972567\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.061452\tEvaluation:\tLoss: 0.052525\tAccuracy: 0.982983\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.041809\tEvaluation:\tLoss: 0.037520\tAccuracy: 0.987750\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.028741\tEvaluation:\tLoss: 0.032990\tAccuracy: 0.988933\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.019630\tEvaluation:\tLoss: 0.023432\tAccuracy: 0.992400\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.013384\tEvaluation:\tLoss: 0.015290\tAccuracy: 0.995367\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.009523\tEvaluation:\tLoss: 0.012608\tAccuracy: 0.996000\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.006746\tEvaluation:\tLoss: 0.009360\tAccuracy: 0.997450\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.004908\tEvaluation:\tLoss: 0.006260\tAccuracy: 0.998683\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.003400\tEvaluation:\tLoss: 0.005119\tAccuracy: 0.998933\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.002427\tEvaluation:\tLoss: 0.003601\tAccuracy: 0.999500\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.001907\tEvaluation:\tLoss: 0.002832\tAccuracy: 0.999683\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.001573\tEvaluation:\tLoss: 0.002267\tAccuracy: 0.999817\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.001345\tEvaluation:\tLoss: 0.001874\tAccuracy: 0.999867\n",
      "Evaluation:\tLoss: 0.067661\tAccuracy: 0.983000\n"
     ]
    }
   ],
   "source": [
    "momentum = SGDMomentum(l_rate=0.03)\n",
    "prepare_and_train(300, momentum, 15, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.191799\tEvaluation:\tLoss: 0.100492\tAccuracy: 0.970867\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.085568\tEvaluation:\tLoss: 0.068238\tAccuracy: 0.980450\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.060908\tEvaluation:\tLoss: 0.051108\tAccuracy: 0.985833\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.047301\tEvaluation:\tLoss: 0.041262\tAccuracy: 0.989017\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.038246\tEvaluation:\tLoss: 0.034530\tAccuracy: 0.991167\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.031656\tEvaluation:\tLoss: 0.029239\tAccuracy: 0.992983\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.026592\tEvaluation:\tLoss: 0.025139\tAccuracy: 0.994533\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.022716\tEvaluation:\tLoss: 0.021847\tAccuracy: 0.995567\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.019624\tEvaluation:\tLoss: 0.019204\tAccuracy: 0.996117\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.017117\tEvaluation:\tLoss: 0.016755\tAccuracy: 0.996900\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.015020\tEvaluation:\tLoss: 0.014788\tAccuracy: 0.997433\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.013293\tEvaluation:\tLoss: 0.013069\tAccuracy: 0.998050\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.011830\tEvaluation:\tLoss: 0.011701\tAccuracy: 0.998500\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.010555\tEvaluation:\tLoss: 0.010489\tAccuracy: 0.998783\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.009515\tEvaluation:\tLoss: 0.009480\tAccuracy: 0.998983\n",
      "Evaluation:\tLoss: 0.065588\tAccuracy: 0.980800\n"
     ]
    }
   ],
   "source": [
    "adagrad = AdaGrad(l_rate=0.03)\n",
    "prepare_and_train(300, adagrad, 15, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.193305\tEvaluation:\tLoss: 0.128659\tAccuracy: 0.964233\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.106336\tEvaluation:\tLoss: 0.110544\tAccuracy: 0.972983\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.083849\tEvaluation:\tLoss: 0.109912\tAccuracy: 0.975050\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.070485\tEvaluation:\tLoss: 0.077022\tAccuracy: 0.983033\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.058701\tEvaluation:\tLoss: 0.071224\tAccuracy: 0.983483\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.049542\tEvaluation:\tLoss: 0.059207\tAccuracy: 0.986667\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.041054\tEvaluation:\tLoss: 0.059214\tAccuracy: 0.987967\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.038832\tEvaluation:\tLoss: 0.050407\tAccuracy: 0.989467\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.030383\tEvaluation:\tLoss: 0.035719\tAccuracy: 0.992300\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.028149\tEvaluation:\tLoss: 0.051083\tAccuracy: 0.990267\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.025770\tEvaluation:\tLoss: 0.044923\tAccuracy: 0.990833\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.021242\tEvaluation:\tLoss: 0.033768\tAccuracy: 0.992983\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.019662\tEvaluation:\tLoss: 0.073625\tAccuracy: 0.987917\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.020756\tEvaluation:\tLoss: 0.024259\tAccuracy: 0.994817\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.017507\tEvaluation:\tLoss: 0.034736\tAccuracy: 0.993233\n",
      "Evaluation:\tLoss: 0.285375\tAccuracy: 0.972900\n"
     ]
    }
   ],
   "source": [
    "rmsprop = RMSProp(l_rate=0.003)\n",
    "prepare_and_train(300, rmsprop, 15, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.200224\tEvaluation:\tLoss: 0.152519\tAccuracy: 0.955783\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.097603\tEvaluation:\tLoss: 0.106865\tAccuracy: 0.967367\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.072807\tEvaluation:\tLoss: 0.105009\tAccuracy: 0.970500\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.057607\tEvaluation:\tLoss: 0.052663\tAccuracy: 0.984983\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.048261\tEvaluation:\tLoss: 0.063359\tAccuracy: 0.981733\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.042291\tEvaluation:\tLoss: 0.140686\tAccuracy: 0.969283\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.036080\tEvaluation:\tLoss: 0.063690\tAccuracy: 0.983917\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.037277\tEvaluation:\tLoss: 0.045237\tAccuracy: 0.988783\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.031606\tEvaluation:\tLoss: 0.055943\tAccuracy: 0.987117\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.028309\tEvaluation:\tLoss: 0.054930\tAccuracy: 0.987717\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.028585\tEvaluation:\tLoss: 0.037205\tAccuracy: 0.991067\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.024791\tEvaluation:\tLoss: 0.041177\tAccuracy: 0.991533\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.023591\tEvaluation:\tLoss: 0.026941\tAccuracy: 0.994000\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.020304\tEvaluation:\tLoss: 0.031355\tAccuracy: 0.993617\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.022621\tEvaluation:\tLoss: 0.036487\tAccuracy: 0.992667\n",
      "Evaluation:\tLoss: 0.242808\tAccuracy: 0.975800\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(l_rate=0.003)\n",
    "prepare_and_train(300, adam, 15, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented and tested four mentioned optimizers, the best model is the one that uses momentum with the coefficient of 0.9. It's accuracy is 0.983000, which is better than result produced using optimizer without momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(weights, X, y, lambda_):\n",
    "    z1, a1, z2, a2 = forward_pass(weights, X)\n",
    "    \n",
    "    L = loss(a2, y.T) + lambda_/2.0 * (np.linalg.norm(weights.W1)**2 + np.linalg.norm(weights.W2)**2)\n",
    "    dL = np.matmul(d_softmax(z2).T, d_loss(a2, y.T)[np.newaxis, ...].T).T[0, ...]\n",
    "    grad_W2 = np.matmul(dL, a1.T) + lambda_*weights.W2\n",
    "    grad_b2 = np.mean(dL, axis=1, keepdims=True)\n",
    "    grad_W1 = np.matmul(np.matmul(weights.W2.T, dL) * D_relu(a1), X) + lambda_*weights.W1\n",
    "    grad_b1 = np.mean(np.matmul(weights.W2.T, dL) * D_relu(a1), axis=1, keepdims=True)\n",
    "    \n",
    "    return NetworkWeights(grad_W1, grad_b1, grad_W2, grad_b2), L\n",
    "\n",
    "\n",
    "def train_one_batch(X, y, weights, optimizer, lambda_):\n",
    "    grads, L = backward_pass(weights, X, y, lambda_)\n",
    "    \n",
    "    optimizer.apply_grads(grads, weights)\n",
    "\n",
    "    return L\n",
    "\n",
    "def train(X, y, weights, optimizer, epochs, batch_size, lambda_):\n",
    "    n_total = X.shape[0]\n",
    "    n_batches = n_total // batch_size + (1 if n_total % batch_size != 0 else 0)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        L = 0.0\n",
    "        for n_batch in range(1, n_batches+1):\n",
    "            batch_X = X[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            batch_y = y[(n_batch-1)*batch_size : n_batch*batch_size, :]\n",
    "            L += train_one_batch(batch_X, batch_y, weights, optimizer, lambda_)\n",
    "            print('\\rEpoch: {}\\tBatch: {}/{}\\tLoss: {:.6f}\\t'.format(epoch, n_batch, n_batches, L/n_batch), end='')\n",
    "        print('\\rEpoch: {}\\t'.format(epoch), end='')\n",
    "        evaluate(X, y, weights)\n",
    "\n",
    "def prepare_and_train(n_hidden_units, optimizer, epochs, batch_size, lambda_):\n",
    "    weights = initialize_network(n_hidden_units)\n",
    "    train(X_train, y_train, weights, optimizer, epochs, batch_size, lambda_)\n",
    "    evaluate(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.656800\tEvaluation:\tLoss: 0.609535\tAccuracy: 0.848800\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.564667\tEvaluation:\tLoss: 0.470109\tAccuracy: 0.881217\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.553430\tEvaluation:\tLoss: 0.458701\tAccuracy: 0.881517\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.546633\tEvaluation:\tLoss: 0.496962\tAccuracy: 0.872133\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.547818\tEvaluation:\tLoss: 0.504405\tAccuracy: 0.870450\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.549929\tEvaluation:\tLoss: 0.649045\tAccuracy: 0.822633\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.550542\tEvaluation:\tLoss: 0.501558\tAccuracy: 0.862450\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.541836\tEvaluation:\tLoss: 0.590317\tAccuracy: 0.844150\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.539647\tEvaluation:\tLoss: 0.737761\tAccuracy: 0.805917\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.526080\tEvaluation:\tLoss: 0.640187\tAccuracy: 0.827500\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.536138\tEvaluation:\tLoss: 0.608654\tAccuracy: 0.825133\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.534711\tEvaluation:\tLoss: 0.719829\tAccuracy: 0.794750\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.528857\tEvaluation:\tLoss: 0.583982\tAccuracy: 0.852050\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.534392\tEvaluation:\tLoss: 0.805685\tAccuracy: 0.799900\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.541015\tEvaluation:\tLoss: 0.569207\tAccuracy: 0.848883\n",
      "Evaluation:\tLoss: 0.561622\tAccuracy: 0.855300\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 0.928782\tEvaluation:\tLoss: 1.461719\tAccuracy: 0.643367\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 0.741814\tEvaluation:\tLoss: 1.059051\tAccuracy: 0.742067\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 0.723521\tEvaluation:\tLoss: 0.921766\tAccuracy: 0.758850\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 0.722261\tEvaluation:\tLoss: 1.076179\tAccuracy: 0.731200\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 0.733200\tEvaluation:\tLoss: 0.985529\tAccuracy: 0.757167\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 0.722117\tEvaluation:\tLoss: 1.133089\tAccuracy: 0.733683\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 0.718992\tEvaluation:\tLoss: 1.257931\tAccuracy: 0.734483\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 0.722657\tEvaluation:\tLoss: 1.177404\tAccuracy: 0.724733\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 0.725782\tEvaluation:\tLoss: 1.275065\tAccuracy: 0.704633\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 0.725806\tEvaluation:\tLoss: 1.146481\tAccuracy: 0.744267\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 0.718118\tEvaluation:\tLoss: 0.821048\tAccuracy: 0.768867\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 0.728122\tEvaluation:\tLoss: 0.966900\tAccuracy: 0.759267\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 0.725362\tEvaluation:\tLoss: 1.214234\tAccuracy: 0.728817\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 0.722227\tEvaluation:\tLoss: 0.775359\tAccuracy: 0.779683\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 0.720445\tEvaluation:\tLoss: 1.077879\tAccuracy: 0.713267\n",
      "Evaluation:\tLoss: 1.063730\tAccuracy: 0.714600\n",
      "Epoch: 1\tBatch: 1875/1875\tLoss: 1.461506\tEvaluation:\tLoss: 0.982067\tAccuracy: 0.670717\n",
      "Epoch: 2\tBatch: 1875/1875\tLoss: 1.125583\tEvaluation:\tLoss: 1.019563\tAccuracy: 0.650367\n",
      "Epoch: 3\tBatch: 1875/1875\tLoss: 1.117326\tEvaluation:\tLoss: 1.067349\tAccuracy: 0.629133\n",
      "Epoch: 4\tBatch: 1875/1875\tLoss: 1.112801\tEvaluation:\tLoss: 0.928853\tAccuracy: 0.684317\n",
      "Epoch: 5\tBatch: 1875/1875\tLoss: 1.104853\tEvaluation:\tLoss: 0.900830\tAccuracy: 0.678033\n",
      "Epoch: 6\tBatch: 1875/1875\tLoss: 1.105998\tEvaluation:\tLoss: 0.924065\tAccuracy: 0.692650\n",
      "Epoch: 7\tBatch: 1875/1875\tLoss: 1.097462\tEvaluation:\tLoss: 1.069654\tAccuracy: 0.649283\n",
      "Epoch: 8\tBatch: 1875/1875\tLoss: 1.102137\tEvaluation:\tLoss: 0.864957\tAccuracy: 0.695383\n",
      "Epoch: 9\tBatch: 1875/1875\tLoss: 1.093175\tEvaluation:\tLoss: 0.822769\tAccuracy: 0.713183\n",
      "Epoch: 10\tBatch: 1875/1875\tLoss: 1.091029\tEvaluation:\tLoss: 0.842059\tAccuracy: 0.700300\n",
      "Epoch: 11\tBatch: 1875/1875\tLoss: 1.093695\tEvaluation:\tLoss: 0.824989\tAccuracy: 0.708950\n",
      "Epoch: 12\tBatch: 1875/1875\tLoss: 1.091078\tEvaluation:\tLoss: 0.857648\tAccuracy: 0.699800\n",
      "Epoch: 13\tBatch: 1875/1875\tLoss: 1.092729\tEvaluation:\tLoss: 0.795693\tAccuracy: 0.720683\n",
      "Epoch: 14\tBatch: 1875/1875\tLoss: 1.091549\tEvaluation:\tLoss: 0.931953\tAccuracy: 0.674333\n",
      "Epoch: 15\tBatch: 1875/1875\tLoss: 1.091915\tEvaluation:\tLoss: 0.912518\tAccuracy: 0.679067\n",
      "Evaluation:\tLoss: 0.933537\tAccuracy: 0.667500\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(l_rate=0.03)\n",
    "prepare_and_train(300, optimizer, 15, 32, 0.0002)\n",
    "prepare_and_train(300, optimizer, 15, 32, 0.002)\n",
    "prepare_and_train(300, optimizer, 15, 32, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcIVzaTuWT5H"
   },
   "source": [
    "**TASK:**\n",
    "\n",
    "Please submit your code with report on the error rate. You can also compare your results with the MNIST performance results exists on the MNIST website.\n",
    "Please also report the effect of different changes you made in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report:** There were 16 models in total tested for basic SGD optimizer with no L2 penalty. Hyperparameters were taken from grid:\n",
    "- number of hidden units: \\[150, 300\\],\n",
    "- number of epochs: \\[5, 15\\],\n",
    "- learning rate: \\[0.3, 0.01\\],\n",
    "- batch size: \\[16, 32\\].\n",
    "\n",
    "The best model of those 16 was the one with 300 hidden units, trained for 15 epochs with learning rate of 0.3 and batch size of 32.\n",
    "\n",
    "Adding momentum optimization with momentum coefficient of 0.9 produced better model with error rate of 1.7% which compared to numbers presented on the website is pretty decent result.\n",
    "\n",
    "Applying other advanced optimizers: AdaGrad, RMSProp and Adam didn't help the model achieve better results or converge faster. Neither L2 regularization led to a model of lower error rate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
