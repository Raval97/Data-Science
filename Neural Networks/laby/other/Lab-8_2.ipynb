{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(\n",
    "    num_words=10001,  # 10000 + 1 because of `index_from=0`,\n",
    "                      # 0 is for `start_char`, `oov_char` and padding.\n",
    "                      # Turns out that actual most frequent word gets number\n",
    "                      # `index_from + 1`.\n",
    "    start_char=0,\n",
    "    oov_char=0,\n",
    "    index_from=0\n",
    ")  # as in the documentation, `num_words` most frequent words should be kept\n",
    "   # with proper indices (actually `num_words-1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few most frequent words and their indices: [('the', 1), ('and', 2), ('a', 3), ('of', 4)]\n",
      "\n",
      "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "print('Few most frequent words and their indices:', sorted(index.items(), key=lambda x:x[1])[:4])\n",
    "print()\n",
    "print(' '.join([reverse_index.get(i, \"#\") for i in X_train_raw[0][:30]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sequences):\n",
    "    res = np.zeros(shape=(len(sequences), 10001), dtype=int)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        res[i, np.array(sequence)] = 1\n",
    "    return res[:, 1:].copy()  # don't keep 0s - `start_char`,\n",
    "                              # `oov_char` or padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (25000, 10000)\n",
      "X_test.shape  = (25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_data(X_train_raw)\n",
    "X_test = prepare_data(X_test_raw)\n",
    "\n",
    "print(f'{X_train.shape = }')\n",
    "print(f'{X_test.shape  = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use test data for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model without early stopping or batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer3 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model1.add(layer1)\n",
    "model1.add(layer2)\n",
    "model1.add(layer3)\n",
    "model1.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "782/782 [==============================] - 16s 19ms/step - loss: 0.3240 - accuracy: 0.8668 - val_loss: 0.2924 - val_accuracy: 0.8826\n",
      "Epoch 2/8\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2071 - accuracy: 0.9222 - val_loss: 0.2991 - val_accuracy: 0.8840\n",
      "Epoch 3/8\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.1571 - accuracy: 0.9433 - val_loss: 0.3171 - val_accuracy: 0.8802\n",
      "Epoch 4/8\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.1121 - accuracy: 0.9641 - val_loss: 0.3486 - val_accuracy: 0.8769\n",
      "Epoch 5/8\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0724 - accuracy: 0.9778 - val_loss: 0.4555 - val_accuracy: 0.8660\n",
      "Epoch 6/8\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0459 - accuracy: 0.9874 - val_loss: 0.5456 - val_accuracy: 0.8640\n",
      "Epoch 7/8\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0300 - accuracy: 0.9925 - val_loss: 0.6999 - val_accuracy: 0.8660\n",
      "Epoch 8/8\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0205 - accuracy: 0.9948 - val_loss: 0.7948 - val_accuracy: 0.8563\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, y_train, epochs=8,\n",
    "                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer3 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model2.add(layer1)\n",
    "model2.add(layer2)\n",
    "model2.add(layer3)\n",
    "model2.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.3225 - accuracy: 0.8672 - val_loss: 0.2866 - val_accuracy: 0.8850\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.2100 - accuracy: 0.9203 - val_loss: 0.3175 - val_accuracy: 0.8745\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.1661 - accuracy: 0.9389 - val_loss: 0.4068 - val_accuracy: 0.8541\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history2 = model2.fit(X_train, y_train, epochs=10,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model with early stopping and single batch normalization + different batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer4 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model3.add(layer1)\n",
    "model3.add(layer2)\n",
    "model3.add(layer3)\n",
    "model3.add(layer4)\n",
    "model3.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 18ms/step - loss: 0.3383 - accuracy: 0.8531 - val_loss: 0.2994 - val_accuracy: 0.8811\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.2043 - accuracy: 0.9203 - val_loss: 0.3079 - val_accuracy: 0.8718\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.1223 - accuracy: 0.9564 - val_loss: 0.3823 - val_accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history3 = model3.fit(X_train, y_train, epochs=10,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer4 = keras.layers.Dense(units=10, activation='relu')\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model4.add(layer1)\n",
    "model4.add(layer2)\n",
    "model4.add(layer3)\n",
    "model4.add(layer4)\n",
    "model4.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.3500 - accuracy: 0.8482 - val_loss: 0.2975 - val_accuracy: 0.8778\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.2374 - accuracy: 0.9086 - val_loss: 0.3094 - val_accuracy: 0.8767\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.1707 - accuracy: 0.9366 - val_loss: 0.3698 - val_accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history4 = model4.fit(X_train, y_train, batch_size=16, epochs=10,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model with early stopping and batch normalization on top of each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100, activation='relu')\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.Dense(units=50, activation='relu')\n",
    "layer4 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer5 = keras.layers.Dense(units=10, activation='relu')\n",
    "layer6 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model5.add(layer1)\n",
    "model5.add(layer2)\n",
    "model5.add(layer3)\n",
    "model5.add(layer4)\n",
    "model5.add(layer5)\n",
    "model5.add(layer6)\n",
    "model5.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 18ms/step - loss: 0.3434 - accuracy: 0.8520 - val_loss: 0.2912 - val_accuracy: 0.8762\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.2134 - accuracy: 0.9196 - val_loss: 0.3037 - val_accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.1338 - accuracy: 0.9523 - val_loss: 0.3625 - val_accuracy: 0.8657\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history5 = model5.fit(X_train, y_train, epochs=10,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model with early stopping and batch normalization before activation of each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = keras.Sequential()\n",
    "\n",
    "layer1 = keras.layers.Dense(units=100)\n",
    "layer2 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer3 = keras.layers.ReLU()\n",
    "layer4 = keras.layers.Dense(units=50)\n",
    "layer5 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer6 = keras.layers.ReLU()\n",
    "layer7 = keras.layers.Dense(units=10)\n",
    "layer8 = keras.layers.BatchNormalization(epsilon=0.001)\n",
    "layer9 = keras.layers.ReLU()\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "model6.add(layer1)\n",
    "model6.add(layer2)\n",
    "model6.add(layer3)\n",
    "model6.add(layer4)\n",
    "model6.add(layer5)\n",
    "model6.add(layer6)\n",
    "model6.add(layer7)\n",
    "model6.add(layer8)\n",
    "model6.add(layer9)\n",
    "model6.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 18ms/step - loss: 0.3566 - accuracy: 0.8464 - val_loss: 0.3036 - val_accuracy: 0.8725\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.2037 - accuracy: 0.9214 - val_loss: 0.3390 - val_accuracy: 0.8649\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.1305 - accuracy: 0.9519 - val_loss: 0.4056 - val_accuracy: 0.8615\n"
     ]
    }
   ],
   "source": [
    "early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history6 = model6.fit(X_train, y_train, epochs=10,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, m1_train_acc = model1.evaluate(X_train, y_train, verbose=0)\n",
    "_, m2_train_acc = model2.evaluate(X_train, y_train, verbose=0)\n",
    "_, m3_train_acc = model3.evaluate(X_train, y_train, verbose=0)\n",
    "_, m4_train_acc = model4.evaluate(X_train, y_train, verbose=0)\n",
    "_, m5_train_acc = model5.evaluate(X_train, y_train, verbose=0)\n",
    "_, m6_train_acc = model6.evaluate(X_train, y_train, verbose=0)\n",
    "\n",
    "_, m1_test_acc = model1.evaluate(X_test, y_test, verbose=0)\n",
    "_, m2_test_acc = model2.evaluate(X_test, y_test, verbose=0)\n",
    "_, m3_test_acc = model3.evaluate(X_test, y_test, verbose=0)\n",
    "_, m4_test_acc = model4.evaluate(X_test, y_test, verbose=0)\n",
    "_, m5_test_acc = model5.evaluate(X_test, y_test, verbose=0)\n",
    "_, m6_test_acc = model6.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached accuracies are as follows:\n",
      "\n",
      "Model without early stopping or batch normalization:\n",
      "Train: 0.9984\tTest: 0.8563\n",
      "\n",
      "Model with early stopping:\n",
      "Train: 0.9348\tTest: 0.8850\n",
      "\n",
      "Model with early stopping and single batch normalization + different batch sizes:\n",
      "Batch size of 32:\n",
      "Train: 0.9362\tTest: 0.8811\n",
      "Batch size of 16:\n",
      "Train: 0.9277\tTest: 0.8778\n",
      "\n",
      "Model with early stopping and batch normalization on top of each hidden layer:\n",
      "Train: 0.9348\tTest: 0.8762\n",
      "\n",
      "Model with early stopping and batch normalization before activation of each hidden layer:\n",
      "Train: 0.9420\tTest: 0.8725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Reached accuracies are as follows:')\n",
    "print()\n",
    "\n",
    "print('Model without early stopping or batch normalization:')\n",
    "print(f'Train: {m1_train_acc:.4f}\\tTest: {m1_test_acc:.4f}')\n",
    "print()\n",
    "\n",
    "print('Model with early stopping:')\n",
    "print(f'Train: {m2_train_acc:.4f}\\tTest: {m2_test_acc:.4f}')\n",
    "print()\n",
    "\n",
    "print('Model with early stopping and single batch normalization + different batch sizes:')\n",
    "print('Batch size of 32:')\n",
    "print(f'Train: {m3_train_acc:.4f}\\tTest: {m3_test_acc:.4f}')\n",
    "print('Batch size of 16:')\n",
    "print(f'Train: {m4_train_acc:.4f}\\tTest: {m4_test_acc:.4f}')\n",
    "print()\n",
    "\n",
    "print('Model with early stopping and batch normalization on top of each hidden layer:')\n",
    "print(f'Train: {m5_train_acc:.4f}\\tTest: {m5_test_acc:.4f}')\n",
    "print()\n",
    "\n",
    "print('Model with early stopping and batch normalization before activation of each hidden layer:')\n",
    "print(f'Train: {m6_train_acc:.4f}\\tTest: {m6_test_acc:.4f}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model reaches higher train accuracy but lower test accuracy that others because it overfits. Early stopping prevents all following models from doing so, thus their test results are better and the training takes less time. Changing batch size from 32 to 16 seems to lower the performance a bit. Using batch normalization on top of all layers doesn't improve test accuracy much and there's little difference in models that apply batch normalization before or after activation of each layer. The best model is the one that uses only early stopping and no batch normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
